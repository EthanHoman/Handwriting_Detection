{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a1a6b1-c47b-4af2-844a-5f23d76ac3cb",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th><h1>CS 3368</h1><h2>Introduction to Artificial Intelligence</h2>\n",
    "        <h1 style=\"color:maroon;\">Handwriting Detector</h1>\n",
    "        <h2 style=\"color:maroon;\">Third Submission</h2></th>\n",
    "        <th><img src=\"https://www.ttu.edu/traditions/images/raiderstatue.jpg\" width=225 height=116 /></th>\n",
    "        <th><p>Texas Tech University Matador Song</p>\n",
    "            <p>Fight, Matadors, for Tech!<br>\n",
    "                Songs of love we'll sing to thee,<br>\n",
    "                Bear our banners far and wide.<br>\n",
    "                Ever to be our pride,<br>\n",
    "                Fearless champions ever be.<br>\n",
    "                Stand on heights of victory.<br>\n",
    "                Strive for honor evermore.<br>\n",
    "                Long live the Matadors!</p>\n",
    "                <p>Music by Harry Lemaire, words by R.C. Marshall</p></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7f8fb-882d-4098-adea-e82a20733aa5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">Enter the Team Name Here</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Sagar Basavaraju</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Harshit Bhatta</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Junghwan Bae</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Ethan Homan</bf></h4></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507e5be-5fe6-415b-afa0-ed6e1f22f27c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">AI Problem</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>What is the problem on which the team worked for this submission?</bf></h4></li>\n",
    "    <ul>\n",
    "        <li> The main problem I was involved with are improving techniques with standardization of the data and improving convergence patterns within the data.\n",
    "         </li>\n",
    "        <li> Providing key performance metrics for accuracy,precision recall and F1 score and create a structured loop for validation for multiple epochs and improved the learning rate and data augmentation. </li>\n",
    "        <li> Added visualization models to view loss precision and help in analyzing the learning rate of my model. </li>\n",
    "        <li> I worked on the transformation of the CNN that was made using tensorflow to Pytorch for better and clearer hyperparameter tuning. (Harshit Bhatta) </li>\n",
    "        <li> I also worked on retraining the model after hyperparameter tuning, and clearly visualizing loss and accuracy of the model. (Harshit Bhatta)  </li>\n",
    "        <li>The team worked on developing an effective machine learning model to classify handwritten numerals from diverse scripts and languages accurately. This involves tackling challenges such as variations in numeral shapes, differing writing styles, and script diversity. The focus for this submission was to refine our existing CNN architecture by incorporating attention mechanisms and exploring data augmentation techniques to enhance feature extraction and improve the model’s generalization.(Junghwan Bae)</li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Is it the same or a different problem than the problem(s) proposed in the proposal or the second submission?  If changes in scope or other changes to the problem have been made since the proposal or second submission, please explain here.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li> THis problem previously defined is similar to what we originally defined but score and added visuals helps programmer and model ouput and train with correct and accurate metrics.</li>\n",
    "        <li> It is the same problem that was defined in the previous submissions: to develop a CNN for the EMNIST dataset. We worked on improving the existing models but focused o n the same problem as before. (Harshit Bhatta)</li>\n",
    "        <li>The problem remains the same as proposed in the original submission. However, there have been changes in the approach to address this problem:\n",
    "            <p>Attention Mechanisms: These were added to the CNN model to help the model focus on the most relevant features of the handwritten digits, improving accuracy and performance.</p>\n",
    "            <p>Transfer Learning Plans: Based on insights gained during initial trials, the team decided to explore transfer learning for better generalization on multilingual numeral datasets.</p>\n",
    "            <p>Expanded Augmentation Techniques: Advanced data augmentation methods, such as script-specific transformations, were introduced to simulate diverse handwriting styles.</p>(Junghwan Bae)</li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Please summarize here what advances and lessons learned the team made in solving the problem for this submission.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Advances: </h5>\n",
    "            <ul>\n",
    "                <li>The final CNN model was modeled and tested by incorporating various improvements like: adding dropout layers, batch normalizations, etc. to improve performance. (Harshit Bhatta)</li>\n",
    "                <li>The model was also able to be saved, so training everytime isnt required for memory and hardware efficiency. (Harshit Bhatta)</li>\n",
    "                <li>Successfully implemented a CNN model with attention layers that significantly improved the focus on relevant features within handwritten digits.(Junghwan Bae)</li>\n",
    "                <li>Developed an efficient data preprocessing pipeline that includes resizing, normalization, and augmentation, reducing overfitting while improving model accuracy.(Junghwan Bae)</li>\n",
    "                <li>Achieved an accuracy of 85% on the validation dataset, which is competitive but leaves room for further improvements through transfer learning and hyperparameter tuning.(Junghwan Bae)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><h5>Lessons Learned: </h5>\n",
    "            <ul>\n",
    "                <li>A lot of lessons reegarding the importance of hyperparameters in the efficiency and accuracy of the model were explored and learned. (Harshit Bhatta)</li> \n",
    "                <li>Attention mechanisms improve performance but increase computational and memory requirements, necessitating optimization in future iterations.(Junghwan Bae)</li>\n",
    "                <li>Diverse augmentation techniques enhance the model’s robustness but require careful calibration to avoid introducing noise.(Junghwan Bae)</li>\n",
    "                <li>Transfer learning, although not implemented in this submission, is expected to be a key component in achieving state-of-the-art results on multilingual numeral datasets.(Junghwan Bae)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1939f91c-d08c-43f1-8290-9f6e0507e92a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">What other ideas, models, approaches, and/or helpful suggestions have you found since the last team submission related to solving the AI problem or similar problems by others?</h1>\n",
    "\n",
    "<hr>\n",
    "<h2 style=\"color: teal\">From Journal/Conference Papers</h2>\n",
    "\n",
    "<p>Papers from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>Journal and conference papers report on research to solve problems.  They do not necessarily have tutorial instructions, but they do report on many ideas that were used in the past, the ideas used by the authors to solve the problem in the paper, and ideas for future research.  They can help you narrow down quickly promising ideas to use to solve a problem.  Examples of ideas are using A-star search and developing a modified version of A-star search to reduce the state space by sampling the actions.  Normal data processing, such as cleaning data, or normal steps to solve a problem would not be the ideas for which you are searching as you read the paper.</p>\n",
    "<p>Another thing that papers have are results that you can use to compare to your solution so you can see if your solution approach has merit.  If your solution does not do as well as solutions reported in the papers, use the papers to find ideas to improve the solution that you have.</p>\n",
    "<p>You want to look for papers listed in the Scopus database available through the <a href=\"https://ttu-primo.hosted.exlibrisgroup.com/primo-explore/dbsearch?vid=01TTU\">TTU Library's Website</a>, and enter \"Scopus\" into the search box.  Papers listed in Scopus will be in venues recognized by scholars and the papers you use should have at least 5 citations.  In particular, when reading the papers, look for relationships among the solution ideas, how the authors evaluated their solutions, and what promising ideas the authors think could improve their work.</p>\n",
    "\n",
    "<p>Use <a href=\"https://ieee-dataport.org/sites/default/files/analysis/27/IEEE%20Citation%20Guidelines.pdf\">IEEE</a> or <a href=\"https://www.acm.org/publications/authors/reference-formatting\">ACM</a> format for listing the paper references.</p>\n",
    "<ul>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \"PyTorch: An Imperative Style, High-Performance Deep Learning Library,\" in Advances in Neural Information Processing Systems 32 (NeurIPS 2019), Vancouver, Canada, 2019.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>This problems tradeoffs were vastly between the the different solution and problem outcomes from the performance and different tasks between different deep learning frameworks and evaluating efficiencies and opportunities for growth versus dynamic frameworks that were used like Chainer which was less expensive and faster language  or Dynet/Torch which were Flexible models but had losses in performance issues   </li>\n",
    "        <li><h5>Past Solution Ideas: </h5> There were past frameworks such as TensorFlow which used static computation with optimizations but had limiting in terms of debugging and hard for computer researchers to interact with the program. Chainer/DyNet used different frameworks and were more flexible such as including conditional statements buts slower performances with static graphs that were used.    </li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5> Pytorch use for combatance of flexibility in developing of CNN models and in terms of Scalabilities with computational environments for resource gains and utilizing different models and various data sets, and integrations with pythons platforms and environments used.  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5> Have more support for mobile platforms, different techniques in visualization and model frameworks with training set ups and improved cluster usages for large scaled data bases  </li>\n",
    "        <li><h5>Evaluation Ideas:</h5> better training speed in the use of image classification and NLP. Utilizing Cuda framework and more efficient GPU utilization and pythons API and reinforcement learning models.   </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5> helps in understanding the various configuraions for CNN and fully connected layers and utilizing custome data sets such as kaggle in built in pyTorch and more extended use with creations in additonal layers and accuracy for different model architectures </li>\n",
    "    </ul>\n",
    "    <ul>\n",
    "\n",
    "<hr>\n",
    "   <li style=\"color:maroon\"><h4><bf>A. Remaida, A. Moumen, Y. El Bouzekri El Idrissi, and B. Abdellaoui, “Tuning convolutional neural networks hyperparameters for offline handwriting recognition,” Proceedings of the 2nd International Conference on Big Data, Modelling and Machine Learning, pp. 71–76, 2021. doi:10.5220/0010728600003101 </bf></h4></li>\n",
    "   <ul>\n",
    "       <li><h5>Problem:</h5>How can we optimize Convolutional Neural Network (CNN) hyperparameters to improve performance in offline handwriting recognition, specifically for the EMNIST letters dataset, while addressing computational complexity and training time challenges?</li>\n",
    "       <li><h5>Past Solution Ideas:</h5>Manual hyperparameter tuning methods, limited CNN architectures for handwriting recognition, traditional image recognition approaches, standard MNIST dataset with limited character sets, manual feature extraction techniques</li>\n",
    "       <li><h5>Authors' Solution Ideas:</h5>Develop a comprehensive comparative study of CNN hyperparameters for EMNIST letters recognition, explore systematic tuning of network architecture elements including number of layers, neurons, optimizers, and learning rates</li>\n",
    "       <li><h5>Future Promising Solution Ideas:</h5>Investigate automated hyperparameter optimization techniques, reduce computational resources required for CNN architecture search, develop more efficient feature extraction methods for handwriting recognition</li>\n",
    "       <li><h5>Evaluation Ideas:</h5>Comprehensive performance analysis of CNN models on EMNIST letters dataset, comparative assessment of different hyperparameter configurations, measuring classification accuracy and computational efficiency</li>\n",
    "       <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>Advanced CNN hyperparameter tuning methodology, detailed exploration of network architecture impacts, systematic approach to improving handwriting recognition performance</li>\n",
    "   </ul>\n",
    "</ul>\n",
    "<hr>\n",
    "<li style=\"color:maroon\"><h4><bf></bf></h4>[1]F. Sadaf, S. M. Taslim Uddin Raju, and A. Muntakim, “Offline Bangla Handwritten Text Recognition: A Comprehensive Study of Various Deep Learning Approaches,” IEEE Xplore, Dec. 01, 2021. https://ieeexplore.ieee.org/document/9718890\n",
    "‌\n",
    "‌</li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5> The main problem was hcallenging due to the many differences within eachfine grained image styles as the different styles were too casual or subtle to make differences noticable to the person.     </li>\n",
    "        <li><h5>Past Solution Ideas:</h5> Many past solution ideas heavily relied on manual feature extractions such as KNN/Decision trees or SVM algorithms which shows challenges within different visual similarities and through which each model would have a hard time to recognize generalize the data within each extraction.   </li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5> Through the use of CNN's and different textures and features we can focus on each model with distinguishing different writing styles to be easily able to recognize these features. and utilizng diffferent preprocessing and normalization of each model.  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5> Look into more deeper netowrks such as ResNet DenseNet and through attention layers focusing on more specalizedd part of the image for classifying them. Utilizing different techniques such as rotations,generalizations and data augmentation to extrpolate hyperparameters and noise.     </li>\n",
    "        <li><h5>Evaluation Ideas:</h5> Evaluation Ideas include using more standardized metrics such as accuracy,precision, and F1 score to balance these datasets. Analyzing mis-classifications within each pattern assesing scalability issues.     </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  The team would like to use more normalization of pixelated values, augmentation and experiment with more deeper CNN's (ResNet,Densenet for feature learning) and incorporating pre-trained models for improved accuracy. </li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>K. L. Banaszewska and M. Plechawska-Wójcik, \"Comparative analysis of CNN models for handwritten digit recognition,\" Journal of Computer Sciences Institute, vol. 32, pp. 179-185, 2024. [Online]. Available:https://ph.pollub.pl/index.php/jcsi/article/view/6239 </bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem: The paper addresses the challenge of efficiently recognizing handwritten digits with high accuracy while balancing computational resource requirements. It compares the performance of three popular Convolutional Neural Network (CNN) architectures—VGG-16, VGG-19, and AlexNet—on the MNIST dataset to determine the best trade-off between accuracy, computation time, and resource usage. (Junghwan Bae)</h5>  </li>\n",
    "        <li><h5>Past Solution Ideas:\n",
    "                <p>Using shallow CNNs, which provided adequate accuracy for simple datasets like MNIST but failed to generalize well to more complex tasks.</p>\n",
    "                <p>Increasing network depth without proper optimization, which led to overfitting and inefficient resource utilization.</p>\n",
    "                <p>Applying traditional machine learning models like SVM and k-NN, which lacked the scalability of modern deep learning techniques.</p></h5>  </li>\n",
    "        <li><h5>Authors' Solution Ideas:\n",
    "                <p>The authors proposed a comparative study of three deep CNN architectures—VGG-16, VGG-19, and AlexNet—on the MNIST dataset.</p>\n",
    "                <p>They evaluated the trade-offs between depth, accuracy, and computational requirements to identify the most efficient architecture for handwritten digit recognition tasks.</p>\n",
    "                <p>They demonstrated the effectiveness of AlexNet for maximizing accuracy (98.76%) while highlighting VGG-16 and VGG-19 as more computationally efficient alternatives with minor accuracy trade-offs (96.83%-96.94%).</p></h5>  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:\n",
    "                <p>Exploring hybrid architectures that combine the depth of AlexNet with the computational efficiency of VGG-16 and VGG-19 to achieve a balanced model.</p>\n",
    "                <p>Investigating advanced regularization techniques, such as dropout and batch normalization, to further reduce overfitting in deeper networks.</p>\n",
    "                <p>Applying transfer learning from pre-trained models on more diverse datasets for improved performance in multilingual handwritten numeral recognition.</p></h5>  </li>\n",
    "        <li><h5>Evaluation Ideas:\n",
    "                <p>Accuracy, Precision, Recall, and F1-Score: Standard metrics to evaluate the models' performance.</p>\n",
    "                <p>Computation Time and Memory Usage: Assessing the efficiency of the architectures concerning hardware resource constraints.</p>\n",
    "                <p>Dataset Variability: Testing the models on datasets with greater diversity than MNIST to evaluate scalability and generalization.</p></h5>  </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:\n",
    "                <p>Implementing AlexNet's deeper architecture for improved accuracy in our numeral recognition model, particularly for multilingual datasets.</p>\n",
    "                <p>Adopting VGG-16 and VGG-19 as baselines for balancing computational efficiency with recognition performance.</p>\n",
    "                <p>Leveraging the insights on resource trade-offs to optimize our solution for specific hardware configurations.</p></h5>  </li>\n",
    "    </ul>\n",
    "\n",
    "<ul>\n",
    "    <li style=\"color:maroon\"><h4><bf>Akhand, M. A. H., Rahat-Uz-Zaman, M., Hye, S., & Kamal, M. A. S. \"Handwritten Numeral Recognition Integrating Start–End Points Measure with Convolutional Neural Network,\" Electronics, vol. 12, no. 2, pp. 1–18, Jan. 2023. doi: 10.3390/electronics12020472.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  The paper addresses the challenges in recognizing handwritten numerals that are similarly shaped, particularly in Bengali and Devanagari scripts. Traditional Convolutional Neural Network (CNN)-based methods often fail to distinguish between these numerals due to their structural similarities.(Junghwan Bae)</li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  \n",
    "                <p>Principal Component Analysis (PCA), Genetic Algorithms, and K-Means Clustering.</p>\n",
    "                <p>Feature descriptors like Histograms of Oriented Gradients (HOG) and Chain Codes.</p>\n",
    "                <p>CNN models with variations in architecture but limited focus on similarly shaped numeral misclassification</p></li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5>  \n",
    "                <p>SEWM measures the start and end points of numeral strokes to add a discriminative feature.</p>\n",
    "                <p>Combined SEWM's results with CNN's classification, using a confidence threshold (σ0) to resolve ambiguous cases.</p>\n",
    "                <p>Focused on Bengali and Devanagari numerals with datasets containing over 18,000 training samples for each language​</p></li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  \n",
    "                <p>Further integration of writing sequence dynamics to improve discrimination.</p>\n",
    "                <p>Testing SEWM-CNN on other scripts and languages.</p>\n",
    "                <p>Incorporating additional human-writing characteristics like stroke pressure and angle​</p></li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  \n",
    "                <p>Use metrics like precision, recall, and F1-score alongside accuracy.</p>\n",
    "                <p>Compare the effectiveness of the SEWM-CNN against both feature-based and other CNN-based methods using standard datasets.</p>\n",
    "                <p>Analyze specific cases of misclassification and their reduction via SEWM​</p></li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  \n",
    "                <p>The integration of auxiliary features, like SEWM, to improve numeral discrimination in challenging cases.</p>\n",
    "                <p>Using confidence thresholds to incorporate auxiliary decisions for ambiguous classifications.</p>\n",
    "                <p>Employing advanced pre-processing techniques, such as binary thresholding and skeletonization, to enhance feature extraction</p></li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li style=\"color:maroon\"><h4><bf>Pinto-Coelho, L., \"How Artificial Intelligence Is Shaping Medical Imaging Technology: A Survey of Innovations and Applications,\" Bioengineering, vol. 10, no. 1435, pp. 1–21, Dec. 2023. doi: 10.3390/bioengineering10121435.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  The paper discusses the integration of artificial intelligence (AI) into medical imaging to address challenges such as early disease detection, efficient image processing, and personalized treatment planning.(Junghwan Bae)</li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  \n",
    "                <p>Human error in analyzing complex medical images.</p>\n",
    "                <p>Constraints in detecting subtle abnormalities in large datasets using traditional feature engineering</p></li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5>  \n",
    "                <p>Deep learning models like CNNs for image segmentation and feature extraction.</p>\n",
    "                <p>Generative adversarial networks (GANs) for synthetic image generation and enhancement</p>\n",
    "                <p>Vision transformers (ViTs) for handling long-range dependencies in images.</p>\n",
    "                <p>Integration of multi-modal image analysis for improved diagnostic accuracy</p></li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  \n",
    "                <p>Enhanced explainability of AI systems in medical imaging.</p>\n",
    "                <p>Development of hybrid AI models combining CNNs and ViTs for robust segmentation tasks.</p>\n",
    "                <p>Leveraging large annotated datasets and transfer learning for broader application​</p></li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  \n",
    "                <p>Sensitivity, specificity, and accuracy for diagnostic tasks.</p>\n",
    "                <p>Structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) for image quality assessments.</p>\n",
    "                <p>Comparison with baseline models using medical datasets like BraTS and KiTS​</p></li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  \n",
    "                <p>The use of transformers alongside CNNs for better feature extraction in numeral recognition tasks.</p>\n",
    "                <p>Transfer learning methods discussed for training models efficiently on limited multilingual numeral datasets.</p>\n",
    "                <p>Incorporating metrics like SSIM and PSNR to evaluate the quality of preprocessed and augmented numeral images</p></li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li style=\"color:maroon\"><h4><bf>Ahlawat, S., Choudhary, A., Nayyar, A., Singh, S., & Yoon, B. (2020). Improved Handwritten Digit Recognition Using Convolutional Neural Networks (CNN). Sensors, vol. 20, no. 12, pp. 1–18, Jun. 2020. doi: 10.3390/s20123344.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  The paper explores the limitations of traditional handwritten digit recognition systems, such as reliance on handcrafted features and the high computational complexity of ensemble methods, and proposes improvements using a fine-tuned pure CNN architecture.(Junghwan Bae)</li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  \n",
    "                <p>Ensemble CNN architectures combining multiple models for improved recognition accuracy.</p>\n",
    "                <p>Traditional methods such as SVM and recurrent neural networks with dropout regularization.</p>\n",
    "                <p>Use of data augmentation and advanced pooling strategies to improve CNN performance.</p></li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5>  \n",
    "                <p>Proposed a pure CNN architecture that achieves superior accuracy (99.89%) for MNIST digit recognition without the computational overhead of ensemble methods.</p>\n",
    "                <p>Introduced optimized hyperparameters, including kernel size, stride, and dilation, and experimented with different CNN configurations (three-layer and four-layer architectures).</p>\n",
    "                <p>Leveraged the Adam optimizer for fast convergence and adaptive learning rates.</p></li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  \n",
    "                <p>Investigating hybrid architectures like CNN-RNN and CNN-HMM for domain-specific recognition tasks.</p>\n",
    "                <p>Exploring evolutionary algorithms to optimize CNN learning parameters, such as the number of layers and kernel sizes.</p>\n",
    "                <p>Extending the study to multilingual datasets for broader applications.</p></li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  \n",
    "                <p>Used the MNIST dataset as a benchmark, achieving a top recognition rate of 99.89%.</p>\n",
    "                <p>Compared results with other models, demonstrating superior performance with a pure CNN</p>\n",
    "                <p>Metrics included accuracy and computational efficiency, with insights into reducing overfitting through techniques like dropout.</p></li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  \n",
    "                <p>Fine-tuning hyperparameters such as stride and dilation to optimize the CNN for multilingual numeral datasets.</p>\n",
    "                <p>Applying techniques to reduce computational complexity while maintaining accuracy, particularly useful for mobile and resource-constrained applications.</p>\n",
    "                <p>Adopting the Adam optimizer and dropout regularization to enhance performance during training and mitigate overfitting.</p></li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li style=\"color:maroon\"><h4><bf>Gupta, A., Miri, R., & Raja, H., \"Recognition of Automated Hand-written Digits on Document Images Making Use of Machine Learning Techniques,\" European Journal of Engineering and Technology Research, vol. 6, no. 4, pp. 37–44, May 2021. doi: 10.24018/ejers.2021.6.4.2460.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  The paper focuses on improving the recognition of handwritten digit strings in complex document images. Challenges addressed include segmentation of overlapping or touching digits, efficient feature extraction, and increasing the accuracy of recognition models under constrained and noisy conditions.(Junghwan Bae)\n",
    "</li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  \n",
    "                <p>Previous studies primarily used Support Vector Machines (SVM) and Artificial Neural Networks (ANN) for digit recognition, achieving limited accuracy due to their inability to handle overlapping or noisy images effectively.</p>\n",
    "                <p>Basic segmentation approaches like vertical projection histograms and statistical methods were used but failed to address segmentation of connected digits adequately.</p></li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5>  \n",
    "                <p>Water Reservoir Segmentation: Introduced a novel segmentation method using the \"water reservoir principle\" to separate touching and overlapping digits. Reservoir points, calculated by simulating water accumulation at intersecting regions of digits, were used to determine segmentation boundaries effectively.</p>\n",
    "                <p>Hybrid Classifiers: Proposed a combination of CNN, SVM, and ANN classifiers for digit recognition, achieving better accuracy compared to individual methods.</p>\n",
    "                <p>Preprocessing Enhancements: Applied advanced image preprocessing techniques, such as noise reduction and adaptive thresholding, to improve the clarity of input data for better segmentation and recognition.</p></li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  \n",
    "                <p>Extend the water reservoir segmentation technique to handle multi-script datasets.</p>\n",
    "                <p>Employ deep learning techniques, such as attention mechanisms, to improve the recognition of overlapping numerals.</p>\n",
    "                <p>Develop dynamic feature extraction methods that adapt to variations in digit size and style for enhanced recognition.</p></li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  \n",
    "                <p>Assess models using standard metrics like accuracy, precision, and recall.</p>\n",
    "                <p>Compare the performance of hybrid classifiers with standalone CNNs using datasets containing noisy and overlapping digits.</p>\n",
    "                <p>Analyze segmentation accuracy by measuring the effectiveness of the water reservoir method in separating connected numerals.</p></li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  \n",
    "                <p>Water Reservoir Segmentation:Incorporate the water reservoir principle to improve segmentation of connected digits in multilingual numeral datasets.</p>\n",
    "                <p>Hybrid Model Design: Explore combining CNNs with simpler classifiers, such as SVM or ANN, to enhance recognition accuracy for complex datasets.</p>\n",
    "                <p>Preprocessing Techniques:Adopt adaptive thresholding and noise reduction methods to preprocess data for better segmentation and feature extraction.</p></li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<li style=\"color:maroon\"><h4><bf>Amran, A. A., On, C. K., Karim, S. A. A., Hung, L. P., See, C. S., Simon, D., Rossdy, M., and Jing, C., “Bornean Orangutan Nest Classification using Image Enhancement with Convolutional Neural Network and Kernel Multi Support Vector Machine Classifier,” Journal of Advanced Research in Applied Sciences and Engineering Technology, vol. 49, no. 2, pp. 187–204, 2025. DOI: 10.37934/araset.49.2.187204.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:Accurately identifying handwritten digits in low-quality or noisy images presents a significant challenge for machine learning models. Variations in handwriting styles, image quality issues (e.g., noise, lighting), and dataset diversity require robust preprocessing and classification techniques to achieve high accuracy.</h5>  </li>\n",
    "        <li><h5>Past Solution Ideas:•\tUse of Convolutional Neural Networks (CNNs) to extract features and directly classify images.\n",
    "\t•\tData augmentation techniques to artificially expand datasets.</h5>  </li>\n",
    "        <li><h5>Authors' Solution Ideas:\tApply image preprocessing techniques (e.g., sharpening, contrast enhancement, histogram equalization, and background removal) to improve the quality of handwritten samples before model training.\n",
    "\tUtilize CNN architectures, such as ResNet-18, ResNet-50, and Inception ResNet-V2, for extracting meaningful hierarchical features from handwriting images.</h5>  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:Enhance the dataset quality for handwriting detection using advanced preprocessing techniques to address noise, distortions, or incomplete characters.\n",
    "\tExperiment with hybrid models (e.g., CNN for feature extraction and SVM for classification) to improve accuracy and robustness for complex handwriting styles.</h5>  </li>\n",
    "        <li><h5>Evaluation Ideas:\t•\tEvaluate the effectiveness of CNN architectures (e.g., ResNet-18, ResNet-50) for feature extraction in handwriting detection.\n",
    "\t•\tCompare metrics like accuracy, precision, and recall across different approaches (e.g., pure CNN vs. CNN+SVM).</h5>  </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:\t•\tImage Enhancement Techniques: Apply local contrast enhancement, sharpening, and histogram equalization to improve the quality of handwriting images.\n",
    "\t•\tHybrid Models: Combine CNNs for feature extraction with Kernel SVM for better classification accuracy in complex or noisy datasets.</h5>  </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e750733-0c99-440a-9abc-9fa8da8d722d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">From Other Informal Resources</h2>\n",
    "<hr>\n",
    "\n",
    "<p>Informal resources from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>Other informal resources include work that is not published in journals or conferences, such as blog sites, tutorials, posted software, software package websites, generative AI tools, and past class project reports that are available online.  Such resources may have concrete examples of how to do various solutions and have software packages or resources that can be used in the team's solution.  Such information and resources should be used to enable the team to go further and faster than they could have gone if the team had started from scratch.</p>\n",
    "<hr>\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>[1]“MNIST Handwritten Digit Recognition in PyTorch,” nextjournal.com. https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "‌</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples:</h5> Utilizing CNN architecture such as extracting different parameter features such as edges and petterns within images also different hues and marks as noise for the model to look out for or ignore would help in visualization. Utilizing Pytorch Max Pooling for downsampling features and reducing computational overloading. Finally establishing more fully connected layers and using nn.CrossEntropyLoss for loss calculation and optimizations such as using Adam or SGD for validation purposes and training of the model.  </li>\n",
    "        <li><h5>What the team would like to use from this resource:</h5> I would love to incorporate from this resource such as batch normalization and dropout. Also use efficient modes of Data loading and loss functions within pytorch such as ADAM or SGD for training our models.  </li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>[1]R. vaishnav, “Handwritten Digit Recognition Using PyTorch, Get 99.5% accuracy in 20 k parameters.,” Medium, Aug. 13, 2020. https://ravivaishnav20.medium.com/handwritten-digit-recognition-using-pytorch-get-99-5-accuracy-in-20-k-parameters-bcb0a2bdfa09 (accessed Nov. 22, 2024).\n",
    "‌</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples:</h5> Different Techniques such as utilizing 1x1 CNN's and using Global Average pooling phenomen to replace fully connected layers and reduce the hyper parameters and utilizing normalization and CNN layers for better training of those models. Also our team could use the process batch normalization for CNN layers for normalizing training distributions. This resource did give examples of using the 7-layer CNN architecture, data augmentation to each implementation using the MNIST dataset.  </li>\n",
    "        <li><h5>What the team would like to use from this resource:</h5> using pixel normalizations and data augmentation for normalizing data and reducing feature range of values and standarding prcoesses for each pixel value. My team would also benfit from using SGD and momentum for improving learning rates and ensuring model convergence and finally adding in the 1X1 convolutions and global average pooling in design and efficient recognitions to identify patterns within the model.  </li>\n",
    "    </ul>\n",
    "    <li style=\"color:maroon\"><h4><bf>Frank C. Eneh, \"Implementing Image Classification Algorithms,\" Medium, Nov. 22, 2024. https://medium.com/@enendufrankc/implementing-image-classification-algorithms-b052082890c0 (accessed Nov. 22, 2024).</bf></h4></li>\n",
    "   <ul>\n",
    "       <li><h5>Helpful information/code/ideas/examples:</h5>Comprehensive overview of image classification algorithms including Convolutional Neural Networks (CNNs), exploring preprocessing techniques, feature extraction methods, and model architectures for improving handwriting recognition performance. The resource provides insights into deep learning approaches, data augmentation strategies, and techniques for reducing model complexity while maintaining high accuracy.</li>\n",
    "       <li><h5>What the team would like to use from this resource:</h5>We will implement advanced preprocessing techniques for image normalization, focusing on strategies for feature extraction using CNN architectures. Our approach includes methods for reducing model parameters while maintaining high performance, utilizing data augmentation to improve model generalization, and applying optimization techniques for enhanced model training and convergence.</li>\n",
    "   </ul>\n",
    "</ul>\n",
    "\n",
    "<li style=\"color:maroon\"><h4><bf>Medium (Towards Data Science), \"A Comprehensive Guide to Attention Mechanisms in Deep Learning,\" Dec. 2023. [Online]. Available: https://towardsdatascience.com/a-comprehensive-guide-to-attention-mechanisms-in-deep-learning</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples:\n",
    "                <p>The article provides a beginner-friendly explanation of attention mechanisms, breaking down how they allow neural networks to focus on specific parts of the input data.</p>\n",
    "                <p>Explores different types of attention, such as self-attention and cross-attention, and their applications in tasks like sequence-to-sequence modeling and image recognition.</p>\n",
    "                <p>Includes Python snippets demonstrating how attention layers are integrated into neural networks using PyTorch and TensorFlow.</p>\n",
    "                <p>Explains how attention mechanisms enhance feature extraction in CNNs for tasks like image classification and object detection.</p></h5>  </li>\n",
    "        <li><h5>What the team would like to use from this resource:\n",
    "                <p>The team plans to incorporate the attention mechanisms discussed into the numeral recognition model to improve its ability to focus on critical image features.</p>\n",
    "                <p>Leverage self-attention techniques to handle complex patterns in multilingual numeral datasets.</p>\n",
    "                <p>Adapt the provided code examples for integrating attention mechanisms into the existing CNN architecture.</p>\n",
    "                <p>Use insights from cross-attention techniques for potential expansion to multi-modal numeral recognition tasks involving images and metadata.</p></h5>  </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa84f4-a1dd-4df8-8fb2-25088b18e92e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Past Plans and Actual Tasks for the Third Project Assignment Submission</h1>\n",
    "\n",
    "<p>The third project assignment submission is the final status report on how far the team has gotten in solving the problem.  Overall, status reports will include but not be limited to items, such as updates to the problem scope, lessons learned, finding more ideas in the conference and journal paper literature as well as informal resources, data sources found, current solution status and performance, comparison of solution to past approaches, software developed and packages used, hardware used, testing, and consideration of new solution approaches.</p>\n",
    "\n",
    "<p>Each team member should catalog the actual work tasks performed for this submission including specfic papers and resources found, AI solution models proposed, preliminary investigative work on a software prototype, researching evaluation strategies for the team's solution, finding needed data files, interviewing experts, solving subproblems, planning, designing, coding, testing, and anything related to helping the team complete the submission well.</p>\n",
    "\n",
    "<p>Tasks should have enough detail to understand clearly and specifically what was done.  For example, rather than say, \"found and wrote up 2 conference papers\", include the authors, such as \"found and wrote up 2 conference papers by Smith et al, 2022, and Breugrand et al, 2019\" so it is clear which papers were contributed to a submission.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Each team member plans to apply an AI solution to the problem and has the following accomplishments and plans:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: maroon\"><bf>Team Member Name</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Planned Team Member Tasks for Third Submission</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Actual Team Member Tasks for Third Submission</bf></th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Sagar Basavaraju</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Review the MNIST Databased and the dataset and review documentation - Week 1 Set Up Keras and pytorch IDE and install specifc libraries such as Pandas,Numpy, and Matplotlib., 2 to 3 hours</li>\n",
    "                <li>Week 2 Set up preprocessing and Normalizing functions to standardize the data and make more efficient changes to avoid discrpencies within the data and Utilizing processes such as Batching,caching and prefetching the loading the data into memory for efficient data retrieval. , 2 to 3 hours</li>\n",
    "                <li>implement Training and testing sets while utilizing a training model to load data and characterize it using convolusional nueral networks, 1 to 2  hour </li>\n",
    "                <li>Define the layers of your CNN model, such as convolutional layers, activation functions (e.g., ReLU), and pooling layers.Week 4 - Fit the model to the training data and specify the number of epochs. 3 to 4 hours </li>\n",
    "                <li>Week 5 : Create a confusion matrix to visualize classification performance. Experiment with different hyperparameters (learning rate, batch size, number of layers) to improve performance. 2 to 3 hours </li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Tasks planned from the proposal submission\n",
    "                    Reviewed the Emnist dataset architecture and set up the python environment for Pytorch and use of Pandas,matplotlib and numpy, also install pip and latest version of python Implement the data loading and preprocessing procedure and traing the model based on features and utilizing a CNN network for acuracy task, time estimate\n",
    "                    Review the recent journal positions and review research papers and select different appropriate layers using the different convolutional layers and activation functions time , 2 to 3 hours</li>\n",
    "                <li>Task was to first set up the python environment made sure python was updated to 3.12 and followed pytorch documentation correctly to allow for downloading of necessary libraries we need for developing our model also installed kaggle to be used for data set retrieval.,1 to 2 hours</li>\n",
    "                <li>Implemented efficient data compatibility with pytorch model and made sure conversion from CSV file to pytorch was efficient.Created a convolusional layer model that extracts key patterns and features from our data set and ReLU and maxpooling to reduce noise and improve model performance. 4 to 6 hours</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Include Proper Loss and Metrics for covering different metrics on loss,precision, and F1 score and proper validation processes for each epoch. Include Validation sets for proper training to prevent overfitting, 2 to 3 hours</li>\n",
    "                <li>Applying Batch Normalization and dropout and increase depth within CNN and improve rate scheduling for handling convergence, 2 hours</li>\n",
    "                <li>Enforce Data Augmentation and add visual plots to evaluate and visualize loss functions for our metrics of our data. 2 to 3 hours </li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Harshit Bhatta</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Apply basic data augmentation techniques (e.g., rotation, flipping) to slightly improve model accuracy, 3 hours (Week 4)</li>\n",
    "<li>Make minor adjustments to the CNN model (e.g., adding a few more layers) based on initial performance, 4 hours (Week 4)</li>\n",
    "<li>Tune key hyperparameters (learning rate, epochs) using simple trial-and-error, 3 hours (Week 5)</li>\n",
    "<li>Evaluate the model performance on validation data, focusing on accuracy and loss, 4 hours (Week 5)</li>\n",
    "<li>Analyze common misclassifications and suggest easy-to-implement improvements, 3 hours (Week 6)</li>\n",
    "<li>Document final results, lessons learned, and potential areas for improvement in future work, 5 hours (Week 6)</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Prepare development environment (pip, PyTorch, memory requirements), 2-3 days</li>\n",
    "<li>Review and preprocess EMNIST dataset (normalize grey-scale images), 3-4 days</li>\n",
    "<li>Initial model training with train-test set evaluation, 4-5 days</li>\n",
    "<li>Implement new neural network models for efficient performance gathering, 5-6 days</li>\n",
    "<li>Refine CNN model with improved accuracy and loss functions, 4-5 days</li>\n",
    "<li>Enhance hyperparameter tuning strategies, 3-4 days</li>\n",
    "<li>Apply data augmentation techniques, 3-4 days</li>\n",
    "<li>Increase CNN depth and layer complexity, 4-5 days</li>\n",
    "<li>Implement L2 regularization to prevent overfitting, 2-3 days</li>\n",
    "<li>Document final research results and findings, 3-4 days</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Done: Make minor adjustments to the CNN model (e.g., adding a few more layers) based on initial performance, 4 hours (Week 4)</li>\n",
    "                <li>Done: Tune key hyperparameters (learning rate, epochs) using simple trial-and-error, 3 hours (Week 5)</li>\n",
    "                <li>Done: Evaluate the model performance on validation data, focusing on accuracy and loss, 4 hours (Week 5)</li>\n",
    "                <li>Done: Document final results, lessons learned, and potential areas for improvement in future work, 5 hours (Week 6)</li>\n",
    "                <li>Done: Prepare development environment (pip, PyTorch, memory requirements), 2-3 days (Week 1)</li>\n",
    "<li>Done: Initial model training with train-test set evaluation, 4-5 days (Week 1-2)</li>\n",
    "<li>Done: Implement new neural network models for efficient performance gathering, 5-6 days (Week 2-3)</li>\n",
    "<li>Done: Refine CNN model with improved accuracy and loss functions, 4-5 days (Week 3-4)</li>\n",
    "<li>Done: Enhance hyperparameter tuning strategies, 3-4 days (Week 4-5)</li>\n",
    "<li>Done: Apply data augmentation techniques, 3-4 days (Week 5)</li>\n",
    "<li>Done: Increase CNN depth and layer complexity, 4-5 days (Week 5-6)</li>\n",
    "<li>Done: Implement L2 regularization to prevent overfitting, 2-3 days (Week 6)</li>\n",
    "<li>Done: Document final research results and findings, 3-4 days (Week 6)</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Junghwan Bae</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Build a basic CNN architecture for numeral recognition and validate its performance on a simple dataset like MNIST. (Estimated: 8 hours)</li>\n",
    "                <li>Implement data preprocessing steps, including normalization and augmentation, for training the model efficiently. (Estimated: 4 hours)</li>\n",
    "                <li>Train the CNN model and evaluate its baseline performance to identify areas for improvement. (Estimated: 6 hours)</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Advanced Data Augmentation: Apply script-specific transformations such as contrast adjustments, flipping, and rotation to improve the diversity of training data. (Estimated: 3 hours)</li>\n",
    "                <li>Refinement of the CNN Model: Integrate deeper attention layers into the CNN and adjust the network's depth to handle script diversity more effectively. (Estimated: 4 hours)</li>\n",
    "                <li>Hyperparameter Tuning: Systematically tune key hyperparameters such as learning rate, batch size, and dropout rates to optimize model performance. (Estimated: 3 hours)</li>\n",
    "                <li>Evaluation of Model Performance: Evaluate the refined model's accuracy, loss, and ability to handle multilingual numeral recognition using validation datasets. (Estimated: 4 hours)</li>\n",
    "                <li>Analysis of Misclassifications: Analyze cases where the model fails, particularly with challenging numerals, and propose solutions to address these issues. (Estimated: 3 hours)</li>\n",
    "                <li>Documentation of Results: Document the final results, lessons learned, and areas for future improvement, with a focus on attention mechanisms. (Estimated: 5 hours)</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Advanced Data Augmentation:Successfully implemented script-specific transformations, including random rotations, flips, and brightness adjustments, to improve training data variability. This step helped enhance the model's generalization on diverse numeral scripts. (Actual: 3 hours) </li>\n",
    "                <li>Refinement of the CNN Model: Integrated an additional attention mechanism layer into the CNN architecture. Adjustments to the network's depth resulted in improved feature extraction and handling of complex numeral patterns. (Actual: 5 hours)</li>\n",
    "                <li>Hyperparameter Tuning: Conducted multiple trials with varying learning rates (0.001, 0.0005) and dropout rates (0.3, 0.5). Identified optimal hyperparameters that significantly improved validation accuracy. (Actual: 4 hours)</li>\n",
    "                <li>Evaluation of Model Performance: Validated the refined model on multilingual datasets and achieved a notable accuracy improvement, increasing from 85% to 88%. Loss metrics were also reduced, indicating better convergence. (Actual: 4 hours)</li>\n",
    "                <li>Analysis of Misclassifications: Identified common misclassifications involving visually similar numerals (e.g., 6 and 9, or 1 and 7). Proposed future improvements such as integrating character-specific augmentation and hybrid CNN-RNN models. (Actual: 3 hours)</li>\n",
    "                <li>Documentation of Results: Completed a detailed report on the results, including performance metrics, lessons learned, and a roadmap for integrating transfer learning in future iterations. (Actual: 5 hours)</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Ethan Homan</th>\n",
    "        <th><ul>\n",
    "<li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Outline a convolutional neural network to recognize MNIST handwritten digits. Select types of layers, activation functions, and configure the overall structure to enhance learning capability, 5 hours</li>\n",
    "                <li>Set initial values for weights and biases, and choose an optimizer like Adam for efficient training, 3 hours</li>\n",
    "                <li>Implement data loaders for processing and normalizing the MNIST dataset, ensuring data is batched for optimal training, 5 hours</li>\n",
    "                <li>Execute the training loop, incorporating batch normalization to standardize inputs and dropout to avoid overfitting, while adjusting parameters based on training feedback, 5 hours</li>\n",
    "                <li>Regularly monitor loss and accuracy, making necessary adjustments to training strategies to optimize performance,3 hours</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks actually done for the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Reviewed and analyzed the document from \"ImageNet Large Scale Visual Recognition Challenge,\" International Journal of Computer Vision , 2 hours</li>\n",
    "                <li>Reviewed and analyzed the pytorch guide site PyLessons. \"Handwriting Recognition Using PyTorch.\", 1 hour</li>\n",
    "                <li>Created the Team Solution for Handwriting Recognition Using PyTorch, 1 hour</li>\n",
    "                <li>Trained the initial CNN model using PyTorch, implementing dynamic adjustments to learning rates and model parameters, 5 hours</li>\n",
    "                <li>Testing and validation of the trained model against the MNIST test set to evaluate the effectiveness of the implemented modifications and to ensure the robustness of the handwriting recognition capabilities., 2 hours</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>\t•\tExpanded dataset with additional augmentations for real-world handwriting variations (2 hrs).\t</li>\n",
    "                <li>\t•\tRefined CNN architecture with optimized preprocessing and dropout layers (3 hrs).</li>\n",
    "                <li>\t•\tIntegrated learning rate scheduling and regularization techniques for better generalization (2 hrs).</li>\n",
    "                  <li>\t•\tFinalized implementation of CNN model with augmented dataset for training and validation (4 hrs).\n",
    "</li>\n",
    "                <li>\t•\tAnalyzed final model performance using accuracy, precision, recall, and F1-score metrics (2 hrs).\n",
    "</li>\n",
    "                  <li>•\tVisualized confusion matrices and sample misclassifications for interpretability (1 hr).</li>\n",
    "                <li>\t•\tCompiled results and methodology into a detailed report referencing key resources (2 hrs).</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f71f4-6357-4057-8c39-5d8a6d7b800c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Current Solution Status</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffa796-2ef5-46fd-bdf7-1d7774298edb",
   "metadata": {},
   "source": [
    "<h2 style=\"color:teal\">What computer hardware, programming language, main software packages, and data files are recommended to be used to run the software for this submission?</h2>\n",
    "    <ul>\n",
    "        <li><h5 style=\"color:maroon\">Computer Hardware</h5> Laptop with possibility of access into HPCC resources : recommended CPU: Atleast Intel I7 and graphics card with CUDA compatibility accepted</li>\n",
    "        <li><h5 style=\"color:maroon\">Programming Language</h5>Atleast Python 3.12.1 or latest vesion.  </li>\n",
    "        <li><h5 style=\"color:maroon\">Main Software Packages</h5> Pytorch, Keras, TensorFlow, matplotlib,seaborn,numpy,pandas</li>\n",
    "        <li><h5 style=\"color:maroon\">Data</h5> Utilizing kaggle csv files for both train and test files found from kaggle. EMNIST was also imported using pytorch database module in some instances.  </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4681706-eda0-4131-9418-1c803a494611",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">What is the performance of the team's solutions in comparison with results from journal/conference papers and other informal resources since the last submission?</h2>\n",
    "\n",
    "<p>Comparisons from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>For the third submission, each team member should plan to add at least one more reference for comparison of performance regardless of how many are given in submission two.  Performance includes but is not limited to the amount of memory consumed, the order of the algorithms used, computation time, number of operations performed, experimental results on various data sets or trials, and measurements, such as precision, recall, accuracy, F-measure, and cluster purity/silouette.  Graphs, such as the ROC curve, precision/recall curve, scatter plots showing the relationship between two attributes, line charts showing model performance at various training points, and bar charts comparing approaches, may also be used.</p>\n",
    "\n",
    "<p>If a direct comparison is not available, try to find a reference that is similar in nature.  In addition, discuss with the course instructors any difficulty you are having in finding references to compare against.  Other ideas include implementing more solutions to compare against each other.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution - Digit Recognizer</h3>\n",
    "\n",
    "<p>Fill in description of team solution</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\"> Informal Reference : [1]A. Tam, “Handwritten Digit Recognition with LeNet5 Model in PyTorch - MachineLearningMastery.com,” MachineLearningMastery.com, Mar. 07, 2023. https://machinelearningmastery.com/handwritten-digit-recognition-with-lenet5-model-in-pytorch/ (accessed Nov. 22, 2024).\n",
    "‌</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>The reference solution included a more of a complexion in structure and architecture of the model with additional features such as ResNet and deeper neural networks acheiving greater accuracy in model training. The reference also included CrossEntropyLoss() functions which when defined helped for analyzes and better represent score cassification and give an accurate probability of each models performance. The reference solution also included important training procedures that enhanced the learning model such as introducing optimizer function,epochs and zero_grad() which helps updata models parameters and help the model converge better.Finally using pooling functions and functions like nn.Dropout() in reducing overfitting of the functions helping reduce noise of the data. </th>\n",
    "        <th>After utilizing data augmentation, incorporating entropyLoss, reducing outisde noise and reduce the effect of disrupting models in-accurate results, and including additional features and metrics such as f1 score and batch normalization/ rate schedulers etc.which helps prevent overfitting and improves on the reference solution. These added benefits, and more visualizations for the each epoch will improve its performance,accuracy and help the model train from within its overall structure.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution (Harshit Bhatta) - Hyperparameters Tuned CNN model for EMNIST</h3>\n",
    "\n",
    "<p>Our team developed a hyperparameter-tuned Convolutional Neural Network (CNN) for EMNIST character recognition, leveraging PyTorch's flexible deep learning framework. We systematically explored learning rates, network architectures, and regularization techniques through extensive computational experiments, implementing a modular approach that maximizes model performance. By utilizing PyTorch's optimization utilities,and dynamic training loops, we created a neural network capable of precise multi-class character classification across the EMNIST dataset, demonstrating the power of sophisticated hyperparameter optimization strategies.</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">[1] “Tuning of CNN Architecture by CSA for EMNIST Data,” in Advances in Information Communication Technology and Computing Proceedings of AICTC 2019, </h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>The reference solution demonstrates a sophisticated approach to optimizing CNN architecture for the challenging EMNIST dataset, achieving a significant accuracy improvement from 95.4% to 98.7% through Clonal Search Algorithm (CSA) parameter tuning. Their methodology involved meticulous hyperparameter optimization, including adjusting critical parameters such as number of epochs (78), batch size (125), number of convolution layers (6), filter sizes (4x4), and network configuration. The key innovation lies in their systematic approach to hyperparameter tuning, which transformed a conventional CNN with 32 kernels of size 5x5 into a more precise model. By employing CSA optimization, they successfully enhanced the model's performance, highlighting the critical importance of intelligent hyperparameter selection in achieving superior classification accuracy on complex datasets like EMNIST. Whereas the teams accuracy with selected hyperparameters was 87.90%. </th>\n",
    "        <th>The main difference was in the amount of epochs and other computationally demanding parameters that they could maximize/ optimize. With a simple CNN with standard hyperparameter values, and 5 epochs, an accuracy for 87.90% was achieved by the teams solution. The teams model was kept computationally light to be able to train the model on regular team/user laptops/PCs. </th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution(Junghwan Bae) - Enhanced Handwritten Numeral Recognition Using CNN with Attention Mechanisms</h3>\n",
    "\n",
    "<p>Our team’s solution focuses on refining a CNN model by incorporating attention mechanisms to improve handwritten numeral recognition, particularly for multilingual datasets. The model uses convolutional layers for feature extraction and attention layers to prioritize relevant image features, addressing challenges like script diversity and visually similar numerals. With additional data augmentation and hyperparameter tuning, we achieved better generalization and accuracy on validation datasets.</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">Paper/Informal Reference: Improved Handwritten Digit Recognition Using Convolutional Neural Networks (CNN). Accuracy: 99.89% on MNIST.</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Our model achieved an accuracy of 89% on the MNIST dataset, slightly lower than the reference but using additional attention mechanisms. The paper focuses on a pure CNN architecture with optimized hyperparameters.</th>\n",
    "        <th>The reference’s superior accuracy is due to its focus on the simpler MNIST dataset and optimized CNN architecture. Our model incorporates attention mechanisms, which slightly increase computational complexity but aim to improve feature extraction.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Discussion/Summary of Solution Results</h3>\n",
    "\n",
    "<p>Please use this section to use items, such as well-labeled graphs and tables, to show a comparison of all team member solutions against each other and with the paper/informal reference solutions, to show a summary of all solution results, and give a discussion of what could be planned to be done for a hypothetical next submission to continue to improve the performance of each team member's solution.</p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4558d31-1388-4d5d-9d4e-fda03a5857af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data for team member performance\n",
    "team_members = ['Junghwan Bae', 'Harshit Bhatta', 'Ethan Homan']\n",
    "accuracy = [96.62, 87.90, 98.56]  # Example accuracies for each team member\n",
    "reference_accuracy = [99, 98.7, 99]  # Example reference accuracies for comparison\n",
    "\n",
    "# Bar width for slim bars\n",
    "bar_width = 0.2\n",
    "# Positions for bars\n",
    "x = np.arange(len(team_members))\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot team member accuracy bars\n",
    "bars1 = plt.bar(x - bar_width / 2, accuracy, width=bar_width, color='skyblue', label='Team Member Accuracy')\n",
    "# Plot reference accuracy bars\n",
    "bars2 = plt.bar(x + bar_width / 2, reference_accuracy, width=bar_width, color='orange', label='Reference Accuracy')\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bar in bars1:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{bar.get_height():.2f}%', ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{bar.get_height():.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Team Members')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy Comparison Across Team Member Solutions', pad=20)  # Adjust title position\n",
    "plt.xticks(x, team_members)\n",
    "plt.ylim(0, 100)  # Setting y-axis from 0 to 100\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac446cf7-469f-4e97-a557-ef827cf49ae6",
   "metadata": {},
   "source": [
    " <h1 style=\"color: teal\">Discussion for further improvement for Team Solutions: </h1>\n",
    "\n",
    "#### 1. Harshit Bhatta: More epochs could be performed on the training of the model by making use of the HPCC, and hyperparameters could be optimized after analyzing the optimal number of epochs, to then tune other parameters to support this direction of improvement for futher development/improvement of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e79bd9-e8dc-4769-b9ab-d709b16918a6",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"color: darkgoldenrod\">Software</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beed320-23a0-4664-9483-b0e626619d81",
   "metadata": {},
   "source": [
    "<p>Please be sure to submit a requirements.txt file so that the instructors can easily run the code in this Jupyter Notebook.  The requirements.txt file should be in the same working directory as the Jupyter Notebook.  Also, please give any special instructions beyond the normal running of code in a Jupyter Notebook, such as where data files should be placed if not in the working directory of this Jupyter Notebook or if some of the installed software packages have additional requirements beyond \"pip install\".</p>\n",
    "\n",
    "<p>Each code cell should have contextually related code, such as a class, function implementing a major algorithm, or a set of short functions that support a larger function in a subsequent cell.  Code cells should also be present to show the performance/evaluation of a solution through well labeled graphs, tables, and/or performance measure values.</p>\n",
    "\n",
    "<p>The code cells also can be organized by each team member's solution.</p>\n",
    "\n",
    "<p>Each major set of related code cells should have the purpose of the code cells, the paper/informal references used (if any) to develop the code in the code cells, the team members who worked on the code cells, and major changes made to the code in the code cells by team members for this submission.  Changes made in a previous submission should not be included.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2 style=\"color: teal\">Are there any special instructions for running the code in this Jupyter Notebook for this submission?</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>make sure python is up to the latest, and pip has been updated succsessfully.</li>\n",
    "    <li>Make sure torch torchvision pandas matplotlib are installed</li>\n",
    "    <li> All data sets have been populated and loaded correctly and install kaggle within python environment and if using the integrated GPU make sure the correct drivers and python are installed correctly.</li>\n",
    "</ul>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e6faba-e744-4216-a5a7-55834d8773b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to capture all of the installed packages so far (run by the team to submit with the Jupyter notebook)\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9b9ece-5297-4cb8-a663-7587f16459d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_5blio7a88h/croot/aext-assistant_1717062156186/work (from -r requirements.txt (line 2))\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_5blio7a88h/croot/aext-assistant_1717062156186/work'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#to install all of the packages in requirements.txt (run by the instructors when grading the notebook)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298cad8-bed1-4d45-aeb2-e6ce5df3a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to import to run the code in the Jupyter Notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565039db-a366-44bf-adf9-f5928380d74a",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Hyperparameters Tuned CNN model for EMNIST</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>This is the source code of this specific team solution to demonstrate a CNN model with tuned hyperparameters to attempt to recognize the characters in the balanced dataset of EMNIST.</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>A. Remaida, A. Moumen, Y. El Bouzekri El Idrissi, and B. Abdellaoui, “Tuning convolutional neural networks hyperparameters for offline handwriting recognition,” Proceedings of the 2nd International Conference on Big Data, Modelling and Machine Learning, pp. 71–76, 2021. doi:10.5220/0010728600003101</li>\n",
    "    <li>Frank C. Eneh, \"Implementing Image Classification Algorithms,\" Medium, Nov. 22, 2024. https://medium.com/@enendufrankc/implementing-image-classification-algorithms-b052082890c0 (accessed Nov. 22, 2024).</li>\n",
    "    <li>[1] “Tuning of CNN Architecture by CSA for EMNIST Data,” in Advances in Information Communication Technology and Computing Proceedings of AICTC 2019,</li>\n",
    "    <li>[1] “CNN: Convolutional Neural Networks Explained - Computerphile,” YouTube, https://www.youtube.com/watch?v=py5byOOHZM8 (accessed Nov. 26, 2024).</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Harshit Bhatta</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/19/2024 - Changed from Tensorflow to Pytorch for model design  and implementation</li>\n",
    "    <li>11/24/2024 - Made final changes for submission: commenting, recheck, and reevaluate.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9222d7b5-d807-4ac8-8bef-f9a777d412fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels shape: torch.Size([64])\n",
      "Labels range: 0 to 45\n",
      "EMNIST_CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1152, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=47, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "[1, 100] loss: 3.124\n",
      "[1, 200] loss: 1.668\n",
      "[1, 300] loss: 1.233\n",
      "[1, 400] loss: 1.064\n",
      "[1, 500] loss: 0.956\n",
      "[1, 600] loss: 0.872\n",
      "[1, 700] loss: 0.799\n",
      "[1, 800] loss: 0.784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    105\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 75\u001b[0m, in \u001b[0;36mEMNIST_CNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))            \u001b[38;5;66;03m#Pooling layer after ReLU activation\u001b[39;00m\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m)               \u001b[38;5;66;03m# Flatten the tensor from (batch_size, 128, 3, 3) to (batch_size, 128 * 3 * 3) for dense layers\u001b[39;00m\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "\n",
    "# importing the required modules \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Defining transformation to normalize the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))     # EMNIST mean and std\n",
    "])\n",
    "\n",
    "\n",
    "#sectioning/ preparing test/train datasets with appropriate transforms\n",
    "train_dataset = datasets.EMNIST(\n",
    "    root='./data',  # Where to store the dataset\n",
    "    split='balanced',  # Choose the split you want\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.EMNIST(\n",
    "    root='./data',\n",
    "    split='balanced',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,  # You can adjust this batch size\n",
    "    shuffle=True    # Shuffle training data\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False   # No need to shuffle test data\n",
    ")\n",
    "\n",
    "# Checking the tensors\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch shape: {images.shape}\")       \n",
    "    print(f\"Labels shape: {labels.shape}\")     \n",
    "    print(f\"Labels range: {labels.min()} to {labels.max()}\")\n",
    "    break  # Checking the first batch\n",
    "\n",
    "\n",
    "# Define the Model\n",
    "class EMNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)        # Defining Convolutional Layers and Setting hyperparameters\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)                    # Using MaxPool layer\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)  # Adjusting for 28x28 input size for the dense layer after convolutional and pooling layers\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 47)  # output class is 47\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))            #Pooling layer after ReLU activation\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 3 * 3)               # Flatten the tensor from (batch_size, 128, 3, 3) to (batch_size, 128 * 3 * 3) for dense layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = EMNIST_CNN()\n",
    "print(model)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()       #Using Cross Entropy as the loss function \n",
    "\n",
    "# Using Adam optimizer for better performance and adaptive learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimal learning rate \"Alpha\" for Adam.  \n",
    "\n",
    "# Number of epochs to train the model. Could be increased according to users need for accuracy and available compute power.\n",
    "num_epochs = 5                          \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients. Resetting this for each batch. \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "# Saving the model to future use with test data/ not burn the training device\n",
    "torch.save(model.state_dict(), 'to_save_model.pth')\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "\n",
    "# Function to load the model later\n",
    "def load_model():\n",
    "    loaded_model = EMNIST_CNN()  # Create a new instance of the model\n",
    "    loaded_model.load_state_dict(torch.load('to_save_model.pth'))\n",
    "    loaded_model.eval()  # Set to evaluation mode\n",
    "    return loaded_model\n",
    "\n",
    "# Evaluate the loaded model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation, not needed for saved model\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "# Function to visualize results\n",
    "def visualize_results(model, test_loader, num_images=10):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    images, labels = next(iter(test_loader))  # Get a batch of test images\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot the images with predicted and actual labels\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(f\"Pred: {predicted[i].item()}\\nActual: {labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the model, evaluate it, and plot its results\n",
    "loaded_model = load_model()\n",
    "evaluate_model(loaded_model, test_loader)\n",
    "visualize_results(loaded_model, test_loader)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1d631-cc96-4958-a65e-d0f32785b506",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Enhanced CNN with Attention Mechanisms</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>Adding additional attention layers to the CNN for better feature extraction.</p>\n",
    "<p>Implementing advanced data augmentation techniques to enhance dataset diversity.</p>\n",
    "<p>Optimizing hyperparameters such as learning rate and dropout rate for improved performance.</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>Paper Reference: Advancing Multilingual Handwritten Numeral Recognition With Attention-Driven Transfer Learning.</li>\n",
    "    <li>Informal Reference: PyTorch Official Documentation, \"Transfer Learning Tutorial.\"</li>\n",
    "    <li>Informal Reference: StatQuest with Josh Starmer, \"Neural Networks Part 8: Image Classification with CNNs.\"</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Junghwan Bae</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/20/2024: Enhanced CNN architecture with additional attention layers.</li>\n",
    "    <li>11/22/2024: Added advanced data augmentation techniques and refined hyperparameters.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7b45c6-a6d2-4dcf-9941-6e15bb2697b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 9.91M/9.91M [00:09<00:00, 1.07MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 295kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1.65M/1.65M [00:01<00:00, 1.21MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 14.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Epoch 1/15 - Loss: 0.5241 - Val Loss: 0.2053 - Accuracy: 0.9362 - Precision: 0.9374 - Recall: 0.9362 - F1 Score: 0.9362\n",
      "Epoch 2/15 - Loss: 0.2382 - Val Loss: 0.1553 - Accuracy: 0.9496 - Precision: 0.9497 - Recall: 0.9496 - F1 Score: 0.9495\n",
      "Epoch 3/15 - Loss: 0.1980 - Val Loss: 0.1304 - Accuracy: 0.9559 - Precision: 0.9562 - Recall: 0.9559 - F1 Score: 0.9559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m\n\u001b[1;32m    101\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Clip gradients to prevent explosion\u001b[39;00m\n\u001b[1;32m    105\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data augmentation and preprocessing to improve generalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(20),  # Randomly rotate images within 20 degrees\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.RandomAffine(10, translate=(0.1, 0.1), scale=(\n",
    "        0.9, 1.1)),  # Apply affine transformations\n",
    "    transforms.RandomAdjustSharpness(2),  # Adjust image sharpness\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    # Normalize images with mean 0.5 and std 0.5\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset with the specified transformations\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Enhanced CNN model with attention mechanisms\n",
    "\n",
    "\n",
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        # First convolutional block with BatchNorm and ReLU activation\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Second convolutional block with BatchNorm and ReLU activation\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Max pooling reduces spatial dimensions by half\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Attention mechanism to focus on important features\n",
    "        self.attention1 = nn.Sequential(\n",
    "            # Compress features to a lower-dimensional space\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64 * 7 * 7),  # Restore original feature dimensions\n",
    "            nn.Sigmoid()  # Output attention weights between 0 and 1\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(256, 10)  # Final output layer for classification\n",
    "        self.dropout = nn.Dropout(0.4)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through first convolutional block\n",
    "        x = self.pool(self.conv1(x))\n",
    "        # Pass through second convolutional block\n",
    "        x = self.pool(self.conv2(x))\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Apply attention mechanism\n",
    "        attention_weights = self.attention1(x)\n",
    "        x = x * attention_weights  # Element-wise multiplication with attention weights\n",
    "        # Pass through the fully connected layers with dropout\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)  # Final layer\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, optimizer, and learning rate scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnhancedCNN().to(device)\n",
    "# Cross-entropy loss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# AdamW optimizer for better regularization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "# Reduce learning rate every 5 epochs\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 15\n",
    "train_losses, val_losses, accuracies, precisions, recalls, f1_scores = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to the same device as the model\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        # Clip gradients to prevent explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()  # Update model parameters\n",
    "        running_loss += loss.item()\n",
    "    train_losses.append(running_loss / len(train_loader)\n",
    "                        )  # Record training loss\n",
    "    scheduler.step()  # Update learning rate\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "        for images, labels in val_loader:\n",
    "            # Move data to the same device as the model\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)  # Get predictions\n",
    "            all_preds.extend(preds.cpu().numpy())  # Store predictions\n",
    "            # Store ground-truth labels\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    val_losses.append(val_loss / len(val_loader))  # Record validation loss\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print metrics for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {running_loss / len(train_loader):.4f} - \"\n",
    "          f\"Val Loss: {val_loss / len(val_loader):.4f} - Accuracy: {accuracy:.4f} - \"\n",
    "          f\"Precision: {precision:.4f} - Recall: {recall:.4f} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy, precision, recall, and F1 score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label='Accuracy')\n",
    "plt.plot(precisions, label='Precision')\n",
    "plt.plot(recalls, label='Recall')\n",
    "plt.plot(f1_scores, label='F1 Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Validation Metrics Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44024dbf-a49d-4901-8d34-cab79c43429c",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Descriptive Name</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>fill in the purpose of the code cell block</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>paper/informal reference</li>\n",
    "    <li>[1]OpenAI, “Introducing ChatGPT,” Openai.com, Nov. 30, 2022. https://openai.com/index/chatgpt/</li>\n",
    "    <li>[1]“torcheval.metrics.functional.multiclass_f1_score — TorchEval main documentation,” Pytorch.org, 2022. https://pytorch.org/torcheval/stable/generated/torcheval.metrics.functional.multiclass_f1_score.html (accessed Nov. 23, 2024).\n",
    "    </li>\n",
    "    <li>[1]GeeksforGeeks, “How to squeeze and unsqueeze a tensor in PyTorch?,” GeeksforGeeks, Mar. 26, 2022. https://www.geeksforgeeks.org/how-to-squeeze-and-unsqueeze-a-tensor-in-pytorch/ (accessed Nov. 23, 2024).\n",
    "‌   </li>\n",
    "    <li>[1]A. Tam, “Handwritten Digit Recognition with LeNet5 Model in PyTorch - MachineLearningMastery.com,” MachineLearningMastery.com, Mar. 07, 2023. https://machinelearningmastery.com/handwritten-digit-recognition-with-lenet5-model-in-pytorch/\n",
    "‌   </li>\n",
    "    <li>[1]“Adam — PyTorch 1.11.0 documentation,” pytorch.org. https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "‌ </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Sagar Basavaraju</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/21/2023 - Found 2 informal and 2 formal research articles regarding CNN networks and different applications and precision values regarding how we can analyze predict and validate and better train our current model</li>\n",
    "    <li>Extensively researching of libraries and optimizations bring important metrics such as F1 Score,accuracy. Using differentiating between labeling and unlabeled data and simulating unlabeled data and extensivly using metrics from sklearn and creating y-pred and y-true values and detailed prediction analyzes based on how well support was compared to true labeled data. </li>\n",
    "</ul>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>mm/dd/yyyy - major change made for this submission</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3f9cba-98c2-4c51-9a4f-488c07be834e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Load Datasets\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomMNISTDataset(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_labeled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomMNISTDataset(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_labeled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mCustomMNISTDataset.__init__\u001b[0;34m(self, name, transform, label_name, is_labeled)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, transform\u001b[38;5;241m=\u001b[39mToTensor(), label_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_labeled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(name)  \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_name \u001b[38;5;241m=\u001b[39m label_name\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "import numpy as np\n",
    "# Custom Dataset\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, name, transform=ToTensor(), label_name=\"label\", is_labeled=True):\n",
    "        self.data = pd.read_csv(name)  \n",
    "        self.transform = transform\n",
    "        self.label_name = label_name\n",
    "        self.is_labeled = is_labeled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pixel_data = self.data.iloc[idx].drop(self.label_name, errors='ignore').values.astype('float32')\n",
    "        scaled_pixel = (pixel_data - pixel_data.min()) / (pixel_data.max() - pixel_data.min())\n",
    "        image = scaled_pixel.reshape(28, 28)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_labeled:\n",
    "            label = int(self.data.iloc[idx][self.label_name])\n",
    "            return image, label\n",
    "        return image\n",
    "\n",
    "# Load Datasets\n",
    "train_dataset = CustomMNISTDataset(name=\"train.csv\", is_labeled=True)\n",
    "test_dataset = CustomMNISTDataset(name=\"test.csv\", is_labeled=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# CNN Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(64 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Training\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        if len(images.shape) != 4:\n",
    "            images = images.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "correct = 0\n",
    "if test_dataset.is_labeled:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if len(images.shape) != 4:\n",
    "                images = images.unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            y_pred.extend(predictions.cpu().numpy().tolist())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "        accuracy = accuracy_score(y_true,y_pred)\n",
    "        F1_score = f1_score(y_true,y_pred,average=\"weighted\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"F1-Score: {F1_score:.2f}%\")\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            if len(images.shape) != 4:\n",
    "                images = images.unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            y_pred.extend(predictions.cpu().numpy().tolist())\n",
    "    #simulations if labels were provided:\n",
    "    nlabels = np.random.choice(10,len(y_pred)) \n",
    "    accuracy = accuracy_score(nlabels,y_pred)\n",
    "    F1_score = f1_score(nlabels,y_pred,average=\"weighted\")\n",
    "    print(f\"Accuracy if no labels: {accuracy:.2f}%\")\n",
    "    print(f\"F1-Score if no labels: {F1_score:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"Predictions (first 10): {y_pred[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d4f87-8aeb-4139-b8fe-81c6999a41c6",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Descriptive Name</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>This code block uses a Convolutional Neural Network to classify handwritten digits from the MNIST dataset. The model preprocesses the dataset applies data augmentation builds the CNN architecture trains it and evaluates its performance which it prints at the end.</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>paper/informal reference</li>\n",
    "    <li>O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, \"ImageNet Large Scale Visual Recognition Challenge,\" International Journal of Computer Vision, vol. 115, no. 3, pp. 211-252, Dec. 2015, doi: 10.1007/s11263-015-0816-y.</li>\n",
    "    <li>David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot et al. 2018. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362, 6419 (2018), 1140-1144.  https://www.science.org/doi/epdf/10.1126/science.aar6404, 4070 citations (Google Scholar), 2276 citations (Scopus).</li>\n",
    "</ul>\n",
    "<li>Amran, A. A., On, C. K., Karim, S. A. A., Hung, L. P., See, C. S., Simon, D., Rossdy, M., and Jing, C., “Bornean Orangutan Nest Classification using Image Enhancement with Convolutional Neural Network and Kernel Multi Support Vector Machine Classifier,” Journal of Advanced Research in Applied Sciences and Engineering Technology, vol. 49, no. 2, pp. 187–204, 2025. [Online]. Available: https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201669656&doi=10.37934%2faraset.49.2.187204&partnerID=40&md5DOI: 10.37934/araset.49.2.187204. DOI: 10.37934/araset.49.2.187204.</li>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/26/2024 - Initial creation of the CNN model architecture and preprocessing pipeline. </li>\n",
    "    <li>11/26/2024 - Integrated data augmentation and early stopping mechanism.</li>\n",
    "</ul>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ad5936-73dc-4478-8ce9-fb23531d1a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 74ms/step - accuracy: 0.7032 - loss: 0.9916 - val_accuracy: 0.3367 - val_loss: 1.9505 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 78ms/step - accuracy: 0.9421 - loss: 0.2629 - val_accuracy: 0.9693 - val_loss: 0.1669 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 79ms/step - accuracy: 0.9568 - loss: 0.2162 - val_accuracy: 0.9815 - val_loss: 0.1313 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 82ms/step - accuracy: 0.9667 - loss: 0.1838 - val_accuracy: 0.9879 - val_loss: 0.1159 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 80ms/step - accuracy: 0.9708 - loss: 0.1695 - val_accuracy: 0.9675 - val_loss: 0.1733 - learning_rate: 0.0010\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9860 - loss: 0.1181\n",
      "Test Accuracy: 98.79%\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# preprocess the data\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype(\"float32\")\n",
    "test_images = test_images.reshape(-1, 28, 28, 1).astype(\"float32\")\n",
    "\n",
    "train_images /= 255.0\n",
    "test_images /= 255.0\n",
    "augmented_train_images = data_augmentation(train_images)\n",
    "\n",
    "# CNN model\n",
    "def build_model():\n",
    "    weight_decay = 1e-4 \n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                      input_shape=(28, 28, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# training\n",
    "history = model.fit(\n",
    "    augmented_train_images, train_labels,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# results\n",
    "def visualize_results(model, test_images, test_labels, num_images=10):\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 2))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
    "        ax.set_title(f\"Pred: {predicted_labels[i]}\\nTrue: {test_labels[i]}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_results(model, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b87e53-ca44-4887-b236-83ffb3056e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
