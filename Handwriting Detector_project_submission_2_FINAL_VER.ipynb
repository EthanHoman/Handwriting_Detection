{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a1a6b1-c47b-4af2-844a-5f23d76ac3cb",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th><h1>CS 3368</h1><h2>Introduction to Artificial Intelligence</h2>\n",
    "        <h1 style=\"color:maroon;\">Team Project Assignment</h1>\n",
    "        <h2 style=\"color:maroon;\">Second Submission</h2></th>\n",
    "        <th><img src=\"https://www.ttu.edu/traditions/images/raiderstatue.jpg\" width=225 height=116 /></th>\n",
    "        <th><p>Texas Tech University Matador Song</p>\n",
    "            <p>Fight, Matadors, for Tech!<br>\n",
    "                Songs of love we'll sing to thee,<br>\n",
    "                Bear our banners far and wide.<br>\n",
    "                Ever to be our pride,<br>\n",
    "                Fearless champions ever be.<br>\n",
    "                Stand on heights of victory.<br>\n",
    "                Strive for honor evermore.<br>\n",
    "                Long live the Matadors!</p>\n",
    "                <p>Music by Harry Lemaire, words by R.C. Marshall</p></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7f8fb-882d-4098-adea-e82a20733aa5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">Handwriting Detector</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Ethan Homan</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Junghwan Bae</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Harshit Bhatta</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Sagar Basavaraju</bf></h4></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507e5be-5fe6-415b-afa0-ed6e1f22f27c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">AI Problem</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>What is the problem on which the team worked for this submission?</bf></h4></li>\n",
    "    <ul>\n",
    "        <li>The core problem I focused on was developing a robust image recognition model that can identify and classify handwritten numerals from multiple languages using PyTorch. Given the complexity of working with multilingual scripts, the challenge was to build a model that could effectively generalize across different numeral shapes and writing styles. I utilized a Convolutional Neural Network (CNN) with attention mechanisms to improve numeral recognition.(Junghwan Bae)</li>\n",
    "        <li>...</li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Is it the same or a different problem than the problem(s) proposed in the proposal?  If changes in scope or other changes to the problem have been made since the proposal, please explain here.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li>The problem remains the same as outlined in the initial proposal: recognizing handwritten numerals from the EMNIST dataset and additional multilingual datasets. The scope has slightly expanded to include attention mechanisms for better performance across diverse scripts. We have refined our approach to include transfer learning and data augmentation techniques, which were identified as necessary during early trials and literature reviews.(Junghwan Bae)</li>\n",
    "        <li>...</li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Please summarize here what advances and lessons learned the team made in solving the problem for this submission.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Advances:</h5>\n",
    "            <ul>\n",
    "                <li>The first major advance was setting up a PyTorch environment configured for transfer learning and incorporating attention mechanisms. This setup allowed for more efficient model training and better handling of script variations.(Junghwan Bae)</li>\n",
    "                <li>I successfully implemented a custom data loader that transforms and normalizes images from various scripts, ensuring compatibility with PyTorch. This preprocessing step also included data augmentation strategies to improve the model's generalizability.(Junghwan Bae)</li>\n",
    "                <li>Designed a CNN model with initial attention mechanisms to better focus on critical features of handwritten numerals. This model was trained on a subset of the multilingual numeral dataset, and preliminary results showed promising improvements in accuracy.(Junghwan Bae)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><h5>Lessons Learned:</h5>\n",
    "            <ul>\n",
    "                <li>A key lesson was the importance of transforming and normalizing input data to optimize the model's performance. Additionally, integrating attention mechanisms early in the design phase proved beneficial for handling diverse scripts.(Junghwan Bae)</li>\n",
    "                <li>Another lesson learned was the necessity of iterative testing and hyperparameter tuning to achieve a balanced performance across different numeral scripts.(Junghwan Bae)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1939f91c-d08c-43f1-8290-9f6e0507e92a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">What other ideas, models, approaches, and/or helpful suggestions have you found since the last team submission related to solving the AI problem or similar problems by others?</h1>\n",
    "\n",
    "<hr>\n",
    "<h2 style=\"color: teal\">From Journal/Conference Papers</h2>\n",
    "\n",
    "<p>Papers from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>Journal and conference papers report on research to solve problems.  They do not necessarily have tutorial instructions, but they do report on many ideas that were used in the past, the ideas used by the authors to solve the problem in the paper, and ideas for future research.  They can help you narrow down quickly promising ideas to use to solve a problem.  Examples of ideas are using A-star search and developing a modified version of A-star search to reduce the state space by sampling the actions.  Normal data processing, such as cleaning data, or normal steps to solve a problem would not be the ideas for which you are searching as you read the paper.</p>\n",
    "<p>Another thing that papers have are results that you can use to compare to your solution so you can see if your solution approach has merit.  If your solution does not do as well as solutions reported in the papers, use the papers to find ideas to improve the solution that you have.</p>\n",
    "<p>You want to look for papers listed in the Scopus database available through the <a href=\"https://ttu-primo.hosted.exlibrisgroup.com/primo-explore/dbsearch?vid=01TTU\">TTU Library's Website</a>, and enter \"Scopus\" into the search box.  Papers listed in Scopus will be in venues recognized by scholars and the papers you use should have at least 5 citations.  In particular, when reading the papers, look for relationships among the solution ideas, how the authors evaluated their solutions, and what promising ideas the authors think could improve their work.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>R. Chinthaginjala et al., “Enhancing Handwritten Text Recognition Accuracy with Gated Mechanisms,” Scientific Reports, vol. 14, no. 16800, pp. 1–12, 2024, doi: 10.1038/s41598-024-67738-8, 1 citation(Scopus)</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  <p>The paper addresses the challenge of recognizing handwritten numerals from multiple languages, focusing on the difficulties posed by script diversity and variations in numeral shapes. These variations make it difficult for standard recognition models to generalize effectively, requiring more sophisticated approaches to handle the nuances across different scripts.</p></li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  <p>Standard CNN Models: Early models used basic Convolutional Neural Networks (CNNs) to perform numeral recognition, which were effective for simple and uniform numeral systems but struggled with multilingual datasets.</p>  \n",
    "        <p>Transfer Learning: To improve generalization, researchers began leveraging transfer learning techniques, where pre-trained models were adapted to numeral recognition tasks. However, these solutions required extensive fine-tuning and often failed to generalize well across numerals from languages with significant structural differences.</p>\n",
    "        <p>Attention Mechanisms: Prior work integrated attention layers into CNNs, allowing models to emphasize important features of the image. These early attention mechanisms provided improvements but lacked optimization for the unique challenges of multilingual handwritten numeral recognition.</p></li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5>  <p>The authors propose an attention-driven transfer learning approach to tackle numeral recognition. By combining the strengths of attention mechanisms and transfer learning, the model can better focus on relevant numeral features while leveraging pre-trained knowledge from related datasets. This dual approach enhances the model's performance across diverse scripts without requiring massive language-specific datasets.</p></li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  <p>The authors suggest exploring domain adaptation techniques to further improve the model's ability to generalize to unseen scripts. Additionally, they propose experimenting with reinforcement learning to enable the model to dynamically adapt its learning process based on the complexity of the numerals it encounters.</p></li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  <p>The proposed model was evaluated using a range of multilingual numeral datasets, with metrics like accuracy, precision, recall, and F1-score. The authors compared their model's performance against traditional CNNs and transfer learning models, demonstrating a significant increase in recognition accuracy. Cross-validation techniques and statistical analysis (e.g., p-values) were used to validate the robustness of the results.</p></li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  <p>Our team intends to incorporate attention mechanisms similar to those described in this paper to improve our model's ability to handle diverse numeral scripts. We also plan to use transfer learning to make our model more efficient, leveraging pre-trained features and minimizing the need for extensive training data. Additionally, we are inspired to explore domain adaptation techniques in future iterations to enhance our model's adaptability.</p></li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Paper reference in IEEE or ACM format, citations</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  </li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  </li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5>  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  </li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  </li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Paper reference in IEEE or ACM format, citations</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>  </li>\n",
    "        <li><h5>Past Solution Ideas:</h5>  </li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5>  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5>  </li>\n",
    "        <li><h5>Evaluation Ideas:</h5>  </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  </li>\n",
    "    </ul>\n",
    "    <li style=\"color:maroon\">...at least one new paper from a SCOPUS referenced publication per team member</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e750733-0c99-440a-9abc-9fa8da8d722d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">From Other Informal Resources</h2>\n",
    "<hr>\n",
    "\n",
    "<p>Informal resources from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>Other informal resources include work that is not published in journals or conferences, such as blog sites, tutorials, posted software, software package websites, generative AI tools, and past class project reports that are available online.  Such resources may have concrete examples of how to do various solutions and have software packages or resources that can be used in the team's solution.  Such information and resources should be used to enable the team to go further and faster than they could have gone if the team had started from scratch.</p>\n",
    "<hr>\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Informal source reference in IEEE or ACM format</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples: (Junghwan Bae)</h5> StatQuest with Josh Starmer, \"Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),\" YouTube, Apr. 12, 2021. [Online]. Available: https://www.youtube.com/watch?v=HGwBXDKFk9I \n",
    "        <p>PyTorch Official Documentation, \"Transfer Learning Tutorial,\" Jan. 2024. [Online]. Available: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html</p> </li>\n",
    "        <li><h5>What the team would like to use from this resource: (Junghwan Bae)</h5> \n",
    "        <p>StatQuest with Josh Starmer, \"Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)\": Our team who are plans to use the clear explanations and step-by-step code examples from this video to improve our understanding and implementation of CNNs in our numeral recognition model. Specifically, we will adopt the techniques discussed for designing effective convolutional and pooling layers, which are essential for feature extraction and reducing overfitting in our model. The insights into handling common image classification challenges will also guide us in optimizing our model's performance.</p> \n",
    "        <p>PyTorch Official Documentation, \"Transfer Learning Tutorial\": Our team member who are using PyTorch intend to leverage the tutorial's practical guide on applying transfer learning to our project. The example of using pre-trained models and fine-tuning them for a new task will help us efficiently train our numeral recognition model without needing extensive training data. We will use this resource to set up our transfer learning workflow and to understand how to customize pre-trained networks for recognizing numerals across multiple languages.</p> \n",
    "        </li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Informal source reference in IEEE or ACM format</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples:</h5>  </li>\n",
    "        <li><h5>What the team would like to use from this resource:</h5>  </li>\n",
    "    </ul>\n",
    "    ...\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa84f4-a1dd-4df8-8fb2-25088b18e92e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Past Plans and Actual Tasks for the Second Submission, and Past/Current Plans for the Third Submission</h1>\n",
    "\n",
    "<p>The second project assignment submission is a status report on how far the team has gotten in solving the problem.  The third project assignment submission is the final status report on how far the team has gotten in solving the problem.  Overall, status reports will include but not be limited to items, such as updates to the problem scope, lessons learned, finding more ideas in the conference and journal paper literature as well as informal resources, data sources found, current solution status and performance, comparison of solution to past approaches, software developed and packages used, hardware used, testing, and consideration of new solution approaches.  Right now, consider that each submission has 3 to 4 weeks of progress that can be planned.</p>\n",
    "\n",
    "<p>The planned tasks each team member should list are those tasks that need to be accomplished to solve the problem using an AI approach.  Each team member will have their own approach to solving the problem and should list tasks, such as investigation, design, considering software packages and computationally strong hardware, finding/processing data, software implementation, evaluation, and analysis of the solution to propose improvements.  The more detail and investigation of what needs to be done that is given, the more realistic the plan and its chance for success to keep the team on target with no surprises (unknown issues arising that could delay progress).  The more general sounding the tasks, the less likely the plan is of any substance to have a chance to work and many surprises could occur to the team while working on the solutions, enough to slow down progress quite a bit.</p>\n",
    "\n",
    "<p>Each team member should catalog the actual work tasks performed for this submission including specfic papers and resources found, AI solution models proposed, preliminary investigative work on a software prototype, researching evaluation strategies for the team's solution, finding needed data files, interviewing experts, solving subproblems, planning, designing, coding, testing, and anything related to helping the team complete the submission well.</p>\n",
    "\n",
    "<p>Tasks should have enough detail to understand clearly and specifically what was done.  For example, rather than say, \"found and wrote up 2 conference papers\", include the authors, such as \"found and wrote up 2 conference papers by Smith et al, 2022, and Breugrand et al, 2019\" so it is clear which papers were contributed to a submission.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Each team member plans to apply an AI solution to the problem and has the following accomplishments and plans:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: maroon\"><bf>Team Member Name</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Planned/Actual Team Member Tasks for Second Submission</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Past/Current Planned Team Member Tasks for Third Submission</bf></th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>team member name</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks actually done for the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, actual time or time estimate</li>\n",
    "                <li>task, actual time or time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li<bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Update on third submission tasks planned as of the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Junghwan Bae</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Investigate the structure and characteristics of the multilingual numeral dataset, focusing on variations in numerals across different languages and scripts. Estimated: 3 hours (Week 1).</li>\n",
    "                <li>Set up a Python environment with PyTorch for transfer learning and attention mechanism integration. Estimated: 2 hours (Week 1).</li>\n",
    "                <li>Implement data loading and preprocessing for the multilingual numeral dataset, including resizing and normalization for model input. Estimated: 4 hours (Week 2).</li>\n",
    "                <li>Design and implement a CNN model with basic attention mechanisms for initial numeral recognition trials. Estimated: 6 hours (Week 2).</li>\n",
    "                <li>Train the CNN model on a subset of the dataset to evaluate basic performance. Estimated: 5 hours (Week 3).</li>\n",
    "                <li>Evaluate the model on a validation set, calculate performance metrics, and identify potential issues. Estimated: 3 hours (Week 3).</li>\n",
    "                <li>Review recent journal papers on multilingual numeral recognition, including techniques from Fateh et al. Estimated: 5 hours (Week 3).</li>\n",
    "                <li>Document progress, challenges, and results, particularly regarding handling different numeral scripts. Estimated: 4 hours (Week 3).</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks actually done for the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Analyzed numeral variations and adjusted preprocessing steps to ensure consistency across scripts. Completed in 3.5 hours.</li>\n",
    "                <li>Successfully configured the Python environment with PyTorch and verified compatibility. Completed in 2 hours.</li>\n",
    "                <li>Developed a data loading pipeline that included resizing, normalization, and batching. Completed in 4 hours.</li>\n",
    "                <li>Created and tested a CNN model with initial attention layers. Encountered challenges with feature extraction. Completed in 6.5 hours.</li>\n",
    "                <li>Conducted training on a dataset subset, monitoring performance with metrics like accuracy. Completed in 5 hours.</li>\n",
    "                <li>Evaluated the model, logging issues such as poor performance on certain scripts. Completed in 3 hours.</li>\n",
    "                <li>Reviewed and documented insights from papers by Fateh et al., applying some techniques. Completed in 5 hours.</li>\n",
    "                <li>Compiled progress and documented key obstacles, including dataset challenges. Completed in 4 hours.</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li<bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Apply advanced data augmentation techniques, such as script-specific transformations, to improve model accuracy. Estimated: 3 hours (Week 4).</li>\n",
    "                <li>Refine the CNN model by integrating deeper attention layers and adjusting network depth to handle script diversity better. Estimated: 4 hours (Week 4).</li>\n",
    "                <li>Conduct systematic hyperparameter tuning, including adjusting the learning rate and dropout rates. Estimated: 3 hours (Week 5).</li>\n",
    "                <li>Evaluate the refined model's performance on validation data, focusing on accuracy, loss, and script handling. Estimated: 4 hours (Week 5).</li>\n",
    "                <li>Analyze common misclassifications and propose targeted improvements. Estimated: 3 hours (Week 6).</li>\n",
    "                <li>Document final results, lessons learned, and areas for future improvement. Estimated: 5 hours (Week 6).</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Update on third submission tasks planned as of the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Implement script-specific data augmentation: Focus on techniques like contrast adjustments, rotations, and flipping specific to numeral scripts. This task is aimed at enhancing model robustness, especially for scripts with high visual complexity. Estimated: 3 hours (Week 4).</li>\n",
    "                <li>Enhance model architecture: Incorporate deeper attention mechanisms and adjust the CNN's network depth to capture more nuanced features of diverse numeral scripts. This involves experimenting with different attention layers to determine the optimal configuration. Estimated: 4 hours (Week 4).</li>\n",
    "                <li>Perform systematic hyperparameter tuning: Adjust critical parameters such as learning rates, dropout rates, and batch sizes using a structured search approach (e.g., grid search or random search) to optimize performance and avoid overfitting. Estimated: 3 hours (Week 5).</li>\n",
    "                <li>Conduct comprehensive model evaluation: Assess the refined model using various performance metrics, including accuracy, precision, recall, and F1-score, on validation datasets. Compare these results with earlier models to measure improvements in handling script diversity. Estimated: 4 hours (Week 5).</li>\n",
    "                <li>Error analysis and targeted improvements: Analyze the types of misclassifications made by the model, focusing on scripts or numeral shapes that are consistently problematic. Propose and implement targeted changes to address these issues, such as enhancing data preprocessing or adjusting network layers. Estimated: 3 hours (Week 6).</li>\n",
    "                <li>Final documentation and lessons learned: Summarize the entire process, from model design to performance outcomes, highlighting key lessons learned. Document potential future research directions and improvements, especially regarding model adaptability and efficiency. Estimated: 5 hours (Week 6).</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>team member name</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks actually done for the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, actual time or time estimate</li>\n",
    "                <li>task, actual time or time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li<bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Update on third submission tasks planned as of the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>team member name</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks actually done for the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, actual time or time estimate</li>\n",
    "                <li>task, actual time or time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li<bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Update on third submission tasks planned as of the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f71f4-6357-4057-8c39-5d8a6d7b800c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Current Solution Status</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffa796-2ef5-46fd-bdf7-1d7774298edb",
   "metadata": {},
   "source": [
    "<h2 style=\"color:teal\">What computer hardware, programming language, main software packages, and data files are recommended to be used to run the software for this submission?</h2>\n",
    "    <ul>\n",
    "        <li><h5 style=\"color:maroon\">Computer Hardware</h5>Such as laptop, processor, memory, disk drives, HPCC, ...</li>\n",
    "        <li><h5 style=\"color:maroon\">Programming Language</h5>Such as Python 3.11, ...</li>\n",
    "        <li><h5 style=\"color:maroon\">Main Software Packages</h5>Such as tensorflow, pytorch, scikit-learn, nltk, ...</li>\n",
    "        <li><h5 style=\"color:maroon\">Data</h5>Such as data file/web link location, data file/self-built, data/software package source, ...</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4681706-eda0-4131-9418-1c803a494611",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">What are the performance results of the team's solutions in comparison with performance results from journal/conference papers and other informal resources?</h2>\n",
    "\n",
    "<p>For the second submission, each team member should give at least one reference to compare against the performance of the solution on which the team member worked.  Performance includes but is not limited to the amount of memory consumed, the order of the algorithms used, computation time, number of operations performed, experimental results on various data sets or trials, and measurements, such as precision, recall, accuracy, F-measure, and cluster purity/silouette.  Graphs, such as the ROC curve, precision/recall curve, scatter plots showing the relationship between two attributes, line charts showing model performance at various training points, and bar charts comparing approaches, may also be used.</p>\n",
    "\n",
    "<p>If a direct comparison is not available, try to find a reference that is similar in nature.  In addition, discuss with the course instructors any difficulty you are having in finding references to compare against.  Other ideas include implementing more solutions to compare against each other.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution - Handwritten Digit Recognition Using CNN and Attention Mechanisms</h3>\n",
    "\n",
    "<p>Our team's solution involves developing a Convolutional Neural Network (CNN) model with integrated attention mechanisms to improve the accuracy of handwritten digit recognition using the MNIST dataset. The model utilizes convolutional layers for feature extraction, followed by attention layers to enhance the focus on critical features. We trained the model for 10 epochs, using data augmentation techniques to improve generalization and combat overfitting. The performance of our model was evaluated using metrics such as accuracy, precision, recall, and F1-score.</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">Paper/Informal Reference</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><p>Reference 1: Advancing Multilingual Handwritten Numeral Recognition With Attention-Driven Transfer Learning - The reference model achieved an accuracy of 90% on a complex, multilingual numeral dataset by leveraging advanced attention mechanisms and transfer learning. Our model, in comparison, achieved an accuracy of 85% on the simpler MNIST dataset, highlighting that our approach is effective but still has room for improvement in handling more complex datasets.</p>\n",
    "        <p>Reference 2: StatQuest with Josh Starmer, \"Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)\" - The concepts from this video helped us design efficient convolutional and pooling layers. While the video didn’t provide specific performance benchmarks, our model's implementation showed significant improvement in feature extraction.</p>\n",
    "        <p>Reference 3: PyTorch Official Documentation, \"Transfer Learning Tutorial\" - While our model currently doesn’t use transfer learning, this tutorial has guided our plans to incorporate it in the future. The reference suggests that transfer learning can significantly boost performance, especially with limited data.</p></th>\n",
    "        <th><p>Our model’s performance is worse than the reference because it lacks the sophisticated transfer learning techniques and domain adaptation used in the paper. Additionally, our current dataset (MNIST) is simpler, and our model architecture is less complex, which may impact performance scalability for more diverse datasets.</p>\n",
    "        <p>The performance is better than a basic CNN implementation because of the incorporation of attention mechanisms, which were inspired by concepts in the StatQuest video. The detailed explanations and examples helped in optimizing the model architecture, reducing overfitting, and improving overall performance.</p>\n",
    "        <p>The performance is currently the same or worse compared to a model using transfer learning because we have not yet implemented this technique. Once transfer learning is integrated, we expect our model to achieve better accuracy and generalization, as suggested by the reference.</p></th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution - Fill in Descriptive Name</h3>\n",
    "\n",
    "<p>Fill in description of team solution</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">Paper/Informal Reference</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fill in reference description solution and performance, and comparison between team's solution and the reference's solution</th>\n",
    "        <th>Fill in reasons for the performance differences or lack of differences between the reference and team solutions</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution - Fill in Descriptive Name</h3>\n",
    "\n",
    "<p>Fill in description of team solution</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">Paper/Informal Reference</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fill in reference description solution and performance, and comparison between team's solution and the reference's solution</th>\n",
    "        <th>Fill in reasons for the performance differences or lack of differences between the reference and team solutions</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution - Fill in Descriptive Name</h3>\n",
    "\n",
    "<p>Fill in description of team solution</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">Paper/Informal Reference</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fill in reference description solution and performance, and comparison between team's solution and the reference's solution</th>\n",
    "        <th>Fill in reasons for the performance differences or lack of differences between the reference and team solutions</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Discussion/Summary of Solution Results</h3>\n",
    "\n",
    "<p>Please use this section to use items, such as well-labeled graphs and tables, to show a comparison of all team member solutions against each other and with the paper/informal reference solutions, to show a summary of all solution results, and give a discussion of what is planned to be done for the next submission to continue to improve the performance of each team member's solution.</p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e79bd9-e8dc-4769-b9ab-d709b16918a6",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"color: darkgoldenrod\">Software</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beed320-23a0-4664-9483-b0e626619d81",
   "metadata": {},
   "source": [
    "<p>Please be sure to submit a requirements.txt file so that the instructors can easily run the code in this Jupyter Notebook.  The requirements.txt file should be in the same working directory as the Jupyter Notebook.  Also, please give any special instructions beyond the normal running of code in a Jupyter Notebook, such as where data files should be placed if not in the working directory of this Jupyter Notebook or if some of the installed software packages have additional requirements beyond \"pip install\".</p>\n",
    "\n",
    "<p>Each code cell should have contextually related code, such as a class, function implementing a major algorithm, or a set of short functions that support a larger function in a subsequent cell.  Code cells should also be present to show the performance/evaluation of a solution through well labeled graphs, tables, and/or performance measure values.</p>\n",
    "\n",
    "<p>The code cells also can be organized by each team member's solution.</p>\n",
    "\n",
    "<p>Each major set of related code cells should have the purpose of the code cells, the paper/informal references used (if any) to develop the code in the code cells, the team members who worked on the code cells, and major changes made to the code in the code cells by team members for this submission.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2 style=\"color: teal\">Are there any special instructions for running the code in this Jupyter Notebook for this submission?</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>Before running the code, make sure you have installed all the required packages listed in requirements.txt</li>\n",
    "    <li>The code is optimized for training on a GPU. If you have a CUDA-compatible GPU, ensure that your CUDA drivers and PyTorch are set up correctly. The code will automatically fall back to the CPU if a GPU is not available, but training will be slower.</li>\n",
    "    <li>The MNIST dataset is downloaded automatically when running the code. Ensure that you have a stable internet connection for the download process.\n",
    "The data will be saved in a ./data directory. Make sure you have permission to create and write to this directory in your working environment.</li>\n",
    "    <li>Execute each code cell in the order presented. This ensures that all necessary variables and models are defined and that dependencies are loaded correctly. Avoid skipping any cells.</li>\n",
    "    <li>If you want to experiment with the model, feel free to adjust the number of epochs, learning rate, or batch size. However, remember to re-run the training and evaluation cells to see the effects of these changes.</li>\n",
    "    <li>The code uses matplotlib to plot sample images. Ensure that matplotlib is installed and that your Jupyter Notebook environment supports inline plotting to view the visualizations correctly.</li>\n",
    "</ul>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6faba-e744-4216-a5a7-55834d8773b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to capture all of the installed packages so far (run by the team to submit with the Jupyter notebook)\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b9ece-5297-4cb8-a663-7587f16459d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to install all of the packages in requirements.txt (run by the instructors when grading the notebook)\n",
    "!pip install -r requirements.txt\n",
    "!pip install -r requirements_BAE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d298cad8-bed1-4d45-aeb2-e6ce5df3a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages to import to run the code in the Jupyter Notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565039db-a366-44bf-adf9-f5928380d74a",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Handwritten Digit Recognition with CNN and Attention</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>The purpose of this code cell block is to build, train, and evaluate a Convolutional Neural Network (CNN) with an integrated attention mechanism for recognizing handwritten digits from the MNIST dataset. The goal is to achieve high accuracy and model efficiency by leveraging attention layers and optimizing hyperparameters.(Junghwan Bae)</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>StatQuest by Josh Starmer, \"Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs),\" YouTube, Apr. 12, 2021. [Online]. Available: https://www.youtube.com/watch?v=HGwBXDKFk9I</li>\n",
    "    <li>PyTorch Official Documentation, \"Transfer Learning Tutorial,\" Jan. 2024. [Online]. Available: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html</li>\n",
    "    <li>A. Fateh et al., “Advancing Multilingual Handwritten Numeral Recognition With Attention-Driven Transfer Learning,” IEEE Access, vol. 12, pp. 41381–41395, 2024. [Online]. Available: https://doi.org/10.1109/ACCESS.2024.3378598</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Junghwan Bae</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/01/2024 - Developed and implemented the CNN model with attention mechanism.</li>\n",
    "    <li>11/02/2024 - Created data preprocessing and loading functions using PyTorch.</li>\n",
    "    <li>11/03/2024 - Conducted model training, validation, and performance evaluation.</li>\n",
    "    <li>11/04/2024 - Integrated visualizations for sample images and model performance metrics.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7b45c6-a6d2-4dcf-9941-6e15bb2697b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "# Import Libraries and Set Up Environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874c8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation and Loading\n",
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637799de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model with Attention Mechanism\n",
    "class CNNWithAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNWithAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),  # Adjusted dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64 * 7 * 7),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Adjusted dimensions\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the linear layer\n",
    "        attention_weights = self.attention(x)\n",
    "        x = x * attention_weights\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62930b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.12827042263764887\n",
      "Epoch 2, Loss: 0.04236825798650583\n",
      "Epoch 3, Loss: 0.029110327315958178\n",
      "Epoch 4, Loss: 0.02073871230374401\n",
      "Epoch 5, Loss: 0.016429805490792204\n",
      "Epoch 6, Loss: 0.012126399752425944\n",
      "Epoch 7, Loss: 0.010637962463308911\n",
      "Epoch 8, Loss: 0.008379923879665576\n",
      "Epoch 9, Loss: 0.009105224151944796\n",
      "Epoch 10, Loss: 0.006926973240145573\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNWithAttention().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04251d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.993\n",
      "Precision: 0.9930185778271765\n",
      "Recall: 0.993\n",
      "F1 Score: 0.9930010579520971\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba6cc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a Sample Image\n",
    "sample_image, sample_label = train_dataset[0]\n",
    "plt.imshow(sample_image.squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"Label: {sample_label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44024dbf-a49d-4901-8d34-cab79c43429c",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Descriptive Name</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>fill in the purpose of the code cell block</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>paper/informal reference</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>mm/dd/yyyy - major change made for this submission</li>\n",
    "    <li>...</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f9cba-98c2-4c51-9a4f-488c07be834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code cells for running and evaluating the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61562ed6",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Descriptive Name</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>fill in the purpose of the code cell block</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>paper/informal reference</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>mm/dd/yyyy - major change made for this submission</li>\n",
    "    <li>...</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a168a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code cells for running and evaluating the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f00be",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Descriptive Name</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>fill in the purpose of the code cell block</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>paper/informal reference</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>mm/dd/yyyy - major change made for this submission</li>\n",
    "    <li>...</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf5db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code cells for running and evaluating the solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
