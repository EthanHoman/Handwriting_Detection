{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a1a6b1-c47b-4af2-844a-5f23d76ac3cb",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <th><h1>CS 3368</h1><h2>Introduction to Artificial Intelligence</h2>\n",
    "        <h1 style=\"color:maroon;\">Team Project Assignment</h1>\n",
    "        <h2 style=\"color:maroon;\">Third Submission</h2></th>\n",
    "        <th><img src=\"https://www.ttu.edu/traditions/images/raiderstatue.jpg\" width=225 height=116 /></th>\n",
    "        <th><p>Texas Tech University Matador Song</p>\n",
    "            <p>Fight, Matadors, for Tech!<br>\n",
    "                Songs of love we'll sing to thee,<br>\n",
    "                Bear our banners far and wide.<br>\n",
    "                Ever to be our pride,<br>\n",
    "                Fearless champions ever be.<br>\n",
    "                Stand on heights of victory.<br>\n",
    "                Strive for honor evermore.<br>\n",
    "                Long live the Matadors!</p>\n",
    "                <p>Music by Harry Lemaire, words by R.C. Marshall</p></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7f8fb-882d-4098-adea-e82a20733aa5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">Enter the Team Name Here</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Sagar Basavaraju</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Harshit Bhatta</bf></h4></li>\n",
    "<li style=\"color:maroon\"><h4><bf>Team member first name and last name</bf></h4></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507e5be-5fe6-415b-afa0-ed6e1f22f27c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">AI Problem</h1>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>What is the problem on which the team worked for this submission?</bf></h4></li>\n",
    "    <ul>\n",
    "        <li> The main problem I was involved with are improving techniques with standardization of the data and improving convergence patterns within the data.\n",
    "         </li>\n",
    "        <li> Providing key performance metrics for accuracy,precision recall and F1 score and create a structured loop for validation for multiple epochs and improved the learning rate and data augmentation. </li>\n",
    "        <li> Added visualization models to view loss precision and help in analyzing the learning rate of my model. </li>\n",
    "        <li> I worked on the transformation of the CNN that was made using tensorflow to Pytorch for better and clearer hyperparameter tuning. (Harshit Bhatta) </li>\n",
    "        <li> I also worked on retraining the model after hyperparameter tuning, and clearly visualizing loss and accuracy of the model. (Harshit Bhatta)  </li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Is it the same or a different problem than the problem(s) proposed in the proposal or the second submission?  If changes in scope or other changes to the problem have been made since the proposal or second submission, please explain here.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li> THis problem previously defined is similar to what we originally defined but score and added visuals helps programmer and model ouput and train with correct and accurate metrics.</li>\n",
    "        <li> It is the same problem that was defined in the previous submissions: to develop a CNN for the EMNIST dataset. We worked on improving the existing models but focused o n the same problem as before. (Harshit Bhatta)</li>\n",
    "        <li>...</li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>Please summarize here what advances and lessons learned the team made in solving the problem for this submission.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Advances: </h5>\n",
    "            <ul>\n",
    "                <li>The final CNN model was modeled and tested by incorporating various improvements like: adding dropout layers, batch normalizations, etc. to improve performance. (Harshit Bhatta)</li>\n",
    "                <li>The model was also able to be saved, so training everytime isnt required for memory and hardware efficiency. (Harshit Bhatta)</li>\n",
    "                <li>Advance...</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><h5>Lessons Learned: </h5>\n",
    "            <ul>\n",
    "                <li>A lot of lessons reegarding the importance of hyperparameters in the efficiency and accuracy of the model were explored and learned. (Harshit Bhatta)</li> \n",
    "                <li>Lesson Learned...</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1939f91c-d08c-43f1-8290-9f6e0507e92a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color:darkgoldenrod\">What other ideas, models, approaches, and/or helpful suggestions have you found since the last team submission related to solving the AI problem or similar problems by others?</h1>\n",
    "\n",
    "<hr>\n",
    "<h2 style=\"color: teal\">From Journal/Conference Papers</h2>\n",
    "\n",
    "<p>Papers from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>Journal and conference papers report on research to solve problems.  They do not necessarily have tutorial instructions, but they do report on many ideas that were used in the past, the ideas used by the authors to solve the problem in the paper, and ideas for future research.  They can help you narrow down quickly promising ideas to use to solve a problem.  Examples of ideas are using A-star search and developing a modified version of A-star search to reduce the state space by sampling the actions.  Normal data processing, such as cleaning data, or normal steps to solve a problem would not be the ideas for which you are searching as you read the paper.</p>\n",
    "<p>Another thing that papers have are results that you can use to compare to your solution so you can see if your solution approach has merit.  If your solution does not do as well as solutions reported in the papers, use the papers to find ideas to improve the solution that you have.</p>\n",
    "<p>You want to look for papers listed in the Scopus database available through the <a href=\"https://ttu-primo.hosted.exlibrisgroup.com/primo-explore/dbsearch?vid=01TTU\">TTU Library's Website</a>, and enter \"Scopus\" into the search box.  Papers listed in Scopus will be in venues recognized by scholars and the papers you use should have at least 5 citations.  In particular, when reading the papers, look for relationships among the solution ideas, how the authors evaluated their solutions, and what promising ideas the authors think could improve their work.</p>\n",
    "\n",
    "<p>Use <a href=\"https://ieee-dataport.org/sites/default/files/analysis/27/IEEE%20Citation%20Guidelines.pdf\">IEEE</a> or <a href=\"https://www.acm.org/publications/authors/reference-formatting\">ACM</a> format for listing the paper references.</p>\n",
    "<ul>\n",
    "\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \"PyTorch: An Imperative Style, High-Performance Deep Learning Library,\" in Advances in Neural Information Processing Systems 32 (NeurIPS 2019), Vancouver, Canada, 2019.</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5>This problems tradeoffs were vastly between the the different solution and problem outcomes from the performance and different tasks between different deep learning frameworks and evaluating efficiencies and opportunities for growth versus dynamic frameworks that were used like Chainer which was less expensive and faster language  or Dynet/Torch which were Flexible models but had losses in performance issues   </li>\n",
    "        <li><h5>Past Solution Ideas: </h5> There were past frameworks such as TensorFlow which used static computation with optimizations but had limiting in terms of debugging and hard for computer researchers to interact with the program. Chainer/DyNet used different frameworks and were more flexible such as including conditional statements buts slower performances with static graphs that were used.    </li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5> Pytorch use for combatance of flexibility in developing of CNN models and in terms of Scalabilities with computational environments for resource gains and utilizing different models and various data sets, and integrations with pythons platforms and environments used.  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5> Have more support for mobile platforms, different techniques in visualization and model frameworks with training set ups and improved cluster usages for large scaled data bases  </li>\n",
    "        <li><h5>Evaluation Ideas:</h5> better training speed in the use of image classification and NLP. Utilizing Cuda framework and more efficient GPU utilization and pythons API and reinforcement learning models.   </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5> helps in understanding the various configuraions for CNN and fully connected layers and utilizing custome data sets such as kaggle in built in pyTorch and more extended use with creations in additonal layers and accuracy for different model architectures </li>\n",
    "    </ul>\n",
    "    <ul>\n",
    "\n",
    "<hr>\n",
    "   <li style=\"color:maroon\"><h4><bf>A. Remaida, A. Moumen, Y. El Bouzekri El Idrissi, and B. Abdellaoui, “Tuning convolutional neural networks hyperparameters for offline handwriting recognition,” Proceedings of the 2nd International Conference on Big Data, Modelling and Machine Learning, pp. 71–76, 2021. doi:10.5220/0010728600003101 </bf></h4></li>\n",
    "   <ul>\n",
    "       <li><h5>Problem:</h5>How can we optimize Convolutional Neural Network (CNN) hyperparameters to improve performance in offline handwriting recognition, specifically for the EMNIST letters dataset, while addressing computational complexity and training time challenges?</li>\n",
    "       <li><h5>Past Solution Ideas:</h5>Manual hyperparameter tuning methods, limited CNN architectures for handwriting recognition, traditional image recognition approaches, standard MNIST dataset with limited character sets, manual feature extraction techniques</li>\n",
    "       <li><h5>Authors' Solution Ideas:</h5>Develop a comprehensive comparative study of CNN hyperparameters for EMNIST letters recognition, explore systematic tuning of network architecture elements including number of layers, neurons, optimizers, and learning rates</li>\n",
    "       <li><h5>Future Promising Solution Ideas:</h5>Investigate automated hyperparameter optimization techniques, reduce computational resources required for CNN architecture search, develop more efficient feature extraction methods for handwriting recognition</li>\n",
    "       <li><h5>Evaluation Ideas:</h5>Comprehensive performance analysis of CNN models on EMNIST letters dataset, comparative assessment of different hyperparameter configurations, measuring classification accuracy and computational efficiency</li>\n",
    "       <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>Advanced CNN hyperparameter tuning methodology, detailed exploration of network architecture impacts, systematic approach to improving handwriting recognition performance</li>\n",
    "   </ul>\n",
    "</ul>\n",
    "<hr>\n",
    "<li style=\"color:maroon\"><h4><bf></bf></h4>[1]F. Sadaf, S. M. Taslim Uddin Raju, and A. Muntakim, “Offline Bangla Handwritten Text Recognition: A Comprehensive Study of Various Deep Learning Approaches,” IEEE Xplore, Dec. 01, 2021. https://ieeexplore.ieee.org/document/9718890\n",
    "‌\n",
    "‌</li>\n",
    "    <ul>\n",
    "        <li><h5>Problem:</h5> The main problem was hcallenging due to the many differences within eachfine grained image styles as the different styles were too casual or subtle to make differences noticable to the person.     </li>\n",
    "        <li><h5>Past Solution Ideas:</h5> Many past solution ideas heavily relied on manual feature extractions such as KNN/Decision trees or SVM algorithms which shows challenges within different visual similarities and through which each model would have a hard time to recognize generalize the data within each extraction.   </li>\n",
    "        <li><h5>Authors' Solution Ideas:</h5> Through the use of CNN's and different textures and features we can focus on each model with distinguishing different writing styles to be easily able to recognize these features. and utilizng diffferent preprocessing and normalization of each model.  </li>\n",
    "        <li><h5>Future Promising Solution Ideas:</h5> Look into more deeper netowrks such as ResNet DenseNet and through attention layers focusing on more specalizedd part of the image for classifying them. Utilizing different techniques such as rotations,generalizations and data augmentation to extrpolate hyperparameters and noise.     </li>\n",
    "        <li><h5>Evaluation Ideas:</h5> Evaluation Ideas include using more standardized metrics such as accuracy,precision, and F1 score to balance these datasets. Analyzing mis-classifications within each pattern assesing scalability issues.     </li>\n",
    "        <li><h5>Ideas the Team Would Like to Use From This Paper:</h5>  The team would like to use more normalization of pixelated values, augmentation and experiment with more deeper CNN's (ResNet,Densenet for feature learning) and incorporating pre-trained models for improved accuracy. </li>\n",
    "    </ul>\n",
    "    <li style=\"color:maroon\">...at least one new paper from a SCOPUS referenced publication per team member</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e750733-0c99-440a-9abc-9fa8da8d722d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">From Other Informal Resources</h2>\n",
    "<hr>\n",
    "\n",
    "<p>Informal resources from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>Other informal resources include work that is not published in journals or conferences, such as blog sites, tutorials, posted software, software package websites, generative AI tools, and past class project reports that are available online.  Such resources may have concrete examples of how to do various solutions and have software packages or resources that can be used in the team's solution.  Such information and resources should be used to enable the team to go further and faster than they could have gone if the team had started from scratch.</p>\n",
    "<hr>\n",
    "<ul>\n",
    "<li style=\"color:maroon\"><h4><bf>[1]“MNIST Handwritten Digit Recognition in PyTorch,” nextjournal.com. https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "‌</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples:</h5> Utilizing CNN architecture such as extracting different parameter features such as edges and petterns within images also different hues and marks as noise for the model to look out for or ignore would help in visualization. Utilizing Pytorch Max Pooling for downsampling features and reducing computational overloading. Finally establishing more fully connected layers and using nn.CrossEntropyLoss for loss calculation and optimizations such as using Adam or SGD for validation purposes and training of the model.  </li>\n",
    "        <li><h5>What the team would like to use from this resource:</h5> I would love to incorporate from this resource such as batch normalization and dropout. Also use efficient modes of Data loading and loss functions within pytorch such as ADAM or SGD for training our models.  </li>\n",
    "    </ul>\n",
    "<li style=\"color:maroon\"><h4><bf>[1]R. vaishnav, “Handwritten Digit Recognition Using PyTorch, Get 99.5% accuracy in 20 k parameters.,” Medium, Aug. 13, 2020. https://ravivaishnav20.medium.com/handwritten-digit-recognition-using-pytorch-get-99-5-accuracy-in-20-k-parameters-bcb0a2bdfa09 (accessed Nov. 22, 2024).\n",
    "‌</bf></h4></li>\n",
    "    <ul>\n",
    "        <li><h5>Helpful information/code/ideas/examples:</h5> Different Techniques such as utilizing 1x1 CNN's and using Global Average pooling phenomen to replace fully connected layers and reduce the hyper parameters and utilizing normalization and CNN layers for better training of those models. Also our team could use the process batch normalization for CNN layers for normalizing training distributions. This resource did give examples of using the 7-layer CNN architecture, data augmentation to each implementation using the MNIST dataset.  </li>\n",
    "        <li><h5>What the team would like to use from this resource:</h5> using pixel normalizations and data augmentation for normalizing data and reducing feature range of values and standarding prcoesses for each pixel value. My team would also benfit from using SGD and momentum for improving learning rates and ensuring model convergence and finally adding in the 1X1 convolutions and global average pooling in design and efficient recognitions to identify patterns within the model.  </li>\n",
    "    </ul>\n",
    "    <li style=\"color:maroon\"><h4><bf>Frank C. Eneh, \"Implementing Image Classification Algorithms,\" Medium, Nov. 22, 2024. https://medium.com/@enendufrankc/implementing-image-classification-algorithms-b052082890c0 (accessed Nov. 22, 2024).</bf></h4></li>\n",
    "   <ul>\n",
    "       <li><h5>Helpful information/code/ideas/examples:</h5>Comprehensive overview of image classification algorithms including Convolutional Neural Networks (CNNs), exploring preprocessing techniques, feature extraction methods, and model architectures for improving handwriting recognition performance. The resource provides insights into deep learning approaches, data augmentation strategies, and techniques for reducing model complexity while maintaining high accuracy.</li>\n",
    "       <li><h5>What the team would like to use from this resource:</h5>We will implement advanced preprocessing techniques for image normalization, focusing on strategies for feature extraction using CNN architectures. Our approach includes methods for reducing model parameters while maintaining high performance, utilizing data augmentation to improve model generalization, and applying optimization techniques for enhanced model training and convergence.</li>\n",
    "   </ul>\n",
    "    ...\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa84f4-a1dd-4df8-8fb2-25088b18e92e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Past Plans and Actual Tasks for the Third Project Assignment Submission</h1>\n",
    "\n",
    "<p>The third project assignment submission is the final status report on how far the team has gotten in solving the problem.  Overall, status reports will include but not be limited to items, such as updates to the problem scope, lessons learned, finding more ideas in the conference and journal paper literature as well as informal resources, data sources found, current solution status and performance, comparison of solution to past approaches, software developed and packages used, hardware used, testing, and consideration of new solution approaches.</p>\n",
    "\n",
    "<p>Each team member should catalog the actual work tasks performed for this submission including specfic papers and resources found, AI solution models proposed, preliminary investigative work on a software prototype, researching evaluation strategies for the team's solution, finding needed data files, interviewing experts, solving subproblems, planning, designing, coding, testing, and anything related to helping the team complete the submission well.</p>\n",
    "\n",
    "<p>Tasks should have enough detail to understand clearly and specifically what was done.  For example, rather than say, \"found and wrote up 2 conference papers\", include the authors, such as \"found and wrote up 2 conference papers by Smith et al, 2022, and Breugrand et al, 2019\" so it is clear which papers were contributed to a submission.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "Each team member plans to apply an AI solution to the problem and has the following accomplishments and plans:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: maroon\"><bf>Team Member Name</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Planned Team Member Tasks for Third Submission</bf></th>\n",
    "        <th style=\"color: maroon\"><bf>Actual Team Member Tasks for Third Submission</bf></th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Sagar Basavaraju</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Review the MNIST Databased and the dataset and review documentation - Week 1 Set Up Keras and pytorch IDE and install specifc libraries such as Pandas,Numpy, and Matplotlib., 2 to 3 hours</li>\n",
    "                <li>Week 2 Set up preprocessing and Normalizing functions to standardize the data and make more efficient changes to avoid discrpencies within the data and Utilizing processes such as Batching,caching and prefetching the loading the data into memory for efficient data retrieval. , 2 to 3 hours</li>\n",
    "                <li>implement Training and testing sets while utilizing a training model to load data and characterize it using convolusional nueral networks, 1 to 2  hour </li>\n",
    "                <li>Define the layers of your CNN model, such as convolutional layers, activation functions (e.g., ReLU), and pooling layers.Week 4 - Fit the model to the training data and specify the number of epochs. 3 to 4 hours </li>\n",
    "                <li>Week 5 : Create a confusion matrix to visualize classification performance. Experiment with different hyperparameters (learning rate, batch size, number of layers) to improve performance. 2 to 3 hours </li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Tasks planned from the proposal submission\n",
    "                    Reviewed the Emnist dataset architecture and set up the python environment for Pytorch and use of Pandas,matplotlib and numpy, also install pip and latest version of python Implement the data loading and preprocessing procedure and traing the model based on features and utilizing a CNN network for acuracy task, time estimate\n",
    "                    Review the recent journal positions and review research papers and select different appropriate layers using the different convolutional layers and activation functions time , 2 to 3 hours</li>\n",
    "                <li>Task was to first set up the python environment made sure python was updated to 3.12 and followed pytorch documentation correctly to allow for downloading of necessary libraries we need for developing our model also installed kaggle to be used for data set retrieval.,1 to 2 hours</li>\n",
    "                <li>Implemented efficient data compatibility with pytorch model and made sure conversion from CSV file to pytorch was efficient.Created a convolusional layer model that extracts key patterns and features from our data set and ReLU and maxpooling to reduce noise and improve model performance. 4 to 6 hours</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Include Proper Loss and Metrics for covering different metrics on loss,precision, and F1 score and proper validation processes for each epoch. Include Validation sets for proper training to prevent overfitting, 2 to 3 hours</li>\n",
    "                <li>Applying Batch Normalization and dropout and increase depth within CNN and improve rate scheduling for handling convergence, 2 hours</li>\n",
    "                <li>Enforce Data Augmentation and add visual plots to evaluate and visualize loss functions for our metrics of our data. 2 to 3 hours </li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>Harshit Bhatta</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Apply basic data augmentation techniques (e.g., rotation, flipping) to slightly improve model accuracy, 3 hours (Week 4)</li>\n",
    "<li>Make minor adjustments to the CNN model (e.g., adding a few more layers) based on initial performance, 4 hours (Week 4)</li>\n",
    "<li>Tune key hyperparameters (learning rate, epochs) using simple trial-and-error, 3 hours (Week 5)</li>\n",
    "<li>Evaluate the model performance on validation data, focusing on accuracy and loss, 4 hours (Week 5)</li>\n",
    "<li>Analyze common misclassifications and suggest easy-to-implement improvements, 3 hours (Week 6)</li>\n",
    "<li>Document final results, lessons learned, and potential areas for improvement in future work, 5 hours (Week 6)</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Prepare development environment (pip, PyTorch, memory requirements), 2-3 days</li>\n",
    "<li>Review and preprocess EMNIST dataset (normalize grey-scale images), 3-4 days</li>\n",
    "<li>Initial model training with train-test set evaluation, 4-5 days</li>\n",
    "<li>Implement new neural network models for efficient performance gathering, 5-6 days</li>\n",
    "<li>Refine CNN model with improved accuracy and loss functions, 4-5 days</li>\n",
    "<li>Enhance hyperparameter tuning strategies, 3-4 days</li>\n",
    "<li>Apply data augmentation techniques, 3-4 days</li>\n",
    "<li>Increase CNN depth and layer complexity, 4-5 days</li>\n",
    "<li>Implement L2 regularization to prevent overfitting, 2-3 days</li>\n",
    "<li>Document final research results and findings, 3-4 days</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>Done: Make minor adjustments to the CNN model (e.g., adding a few more layers) based on initial performance, 4 hours (Week 4)</li>\n",
    "                <li>Done: Tune key hyperparameters (learning rate, epochs) using simple trial-and-error, 3 hours (Week 5)</li>\n",
    "                <li>Done: Evaluate the model performance on validation data, focusing on accuracy and loss, 4 hours (Week 5)</li>\n",
    "                <li>Done: Document final results, lessons learned, and potential areas for improvement in future work, 5 hours (Week 6)</li>\n",
    "                <li>Done: Prepare development environment (pip, PyTorch, memory requirements), 2-3 days (Week 1)</li>\n",
    "<li>Done: Initial model training with train-test set evaluation, 4-5 days (Week 1-2)</li>\n",
    "<li>Done: Implement new neural network models for efficient performance gathering, 5-6 days (Week 2-3)</li>\n",
    "<li>Done: Refine CNN model with improved accuracy and loss functions, 4-5 days (Week 3-4)</li>\n",
    "<li>Done: Enhance hyperparameter tuning strategies, 3-4 days (Week 4-5)</li>\n",
    "<li>Done: Apply data augmentation techniques, 3-4 days (Week 5)</li>\n",
    "<li>Done: Increase CNN depth and layer complexity, 4-5 days (Week 5-6)</li>\n",
    "<li>Done: Implement L2 regularization to prevent overfitting, 2-3 days (Week 6)</li>\n",
    "<li>Done: Document final research results and findings, 3-4 days (Week 6)</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr style=\"text-align:left\">\n",
    "        <th>team member name</th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks planned from the proposal submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            <li><bf>Tasks planned from the second submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            </ul>\n",
    "        </th>\n",
    "        <th><ul>\n",
    "            <li><bf>Tasks actually done for the third submission</bf></li>\n",
    "            <ul>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>task, time estimate</li>\n",
    "                <li>...</li>\n",
    "            </ul>\n",
    "            <br>\n",
    "            </ul>\n",
    "        </th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f71f4-6357-4057-8c39-5d8a6d7b800c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 style=\"color: darkgoldenrod\">Current Solution Status</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffa796-2ef5-46fd-bdf7-1d7774298edb",
   "metadata": {},
   "source": [
    "<h2 style=\"color:teal\">What computer hardware, programming language, main software packages, and data files are recommended to be used to run the software for this submission?</h2>\n",
    "    <ul>\n",
    "        <li><h5 style=\"color:maroon\">Computer Hardware</h5> Laptop with possibility of access into HPCC resources : recommended CPU: Atleast Intel I7 and graphics card with CUDA compatibility accepted</li>\n",
    "        <li><h5 style=\"color:maroon\">Programming Language</h5>Atleast Python 3.12.1 or latest vesion.  </li>\n",
    "        <li><h5 style=\"color:maroon\">Main Software Packages</h5> Pytorch, Keras, TensorFlow, matplotlib,seaborn,numpy,pandas</li>\n",
    "        <li><h5 style=\"color:maroon\">Data</h5> Utilizing kaggle csv files for both train and test files found from kaggle. EMNIST was also imported using pytorch database module in some instances.  </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4681706-eda0-4131-9418-1c803a494611",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2 style=\"color: teal\">What is the performance of the team's solutions in comparison with results from journal/conference papers and other informal resources since the last submission?</h2>\n",
    "\n",
    "<p>Comparisons from previous submissions should not be listed here.</p>\n",
    "\n",
    "<p>For the third submission, each team member should plan to add at least one more reference for comparison of performance regardless of how many are given in submission two.  Performance includes but is not limited to the amount of memory consumed, the order of the algorithms used, computation time, number of operations performed, experimental results on various data sets or trials, and measurements, such as precision, recall, accuracy, F-measure, and cluster purity/silouette.  Graphs, such as the ROC curve, precision/recall curve, scatter plots showing the relationship between two attributes, line charts showing model performance at various training points, and bar charts comparing approaches, may also be used.</p>\n",
    "\n",
    "<p>If a direct comparison is not available, try to find a reference that is similar in nature.  In addition, discuss with the course instructors any difficulty you are having in finding references to compare against.  Other ideas include implementing more solutions to compare against each other.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution - Digit Recognizer</h3>\n",
    "\n",
    "<p>Fill in description of team solution</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\"> Informal Reference : [1]A. Tam, “Handwritten Digit Recognition with LeNet5 Model in PyTorch - MachineLearningMastery.com,” MachineLearningMastery.com, Mar. 07, 2023. https://machinelearningmastery.com/handwritten-digit-recognition-with-lenet5-model-in-pytorch/ (accessed Nov. 22, 2024).\n",
    "‌</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>The reference solution included a more of a complexion in structure and architecture of the model with additional features such as ResNet and deeper neural networks acheiving greater accuracy in model training. The reference also included CrossEntropyLoss() functions which when defined helped for analyzes and better represent score cassification and give an accurate probability of each models performance. The reference solution also included important training procedures that enhanced the learning model such as introducing optimizer function,epochs and zero_grad() which helps updata models parameters and help the model converge better.Finally using pooling functions and functions like nn.Dropout() in reducing overfitting of the functions helping reduce noise of the data. </th>\n",
    "        <th>After utilizing data augmentation, incorporating entropyLoss, reducing outisde noise and reduce the effect of disrupting models in-accurate results, and including additional features and metrics such as f1 score and batch normalization/ rate schedulers etc.which helps prevent overfitting and improves on the reference solution. These added benefits, and more visualizations for the each epoch will improve its performance,accuracy and help the model train from within its overall structure.</th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution (Harshit Bhatta) - Hyperparameters Tuned CNN model for EMNIST</h3>\n",
    "\n",
    "<p>Our team developed a hyperparameter-tuned Convolutional Neural Network (CNN) for EMNIST character recognition, leveraging PyTorch's flexible deep learning framework. We systematically explored learning rates, network architectures, and regularization techniques through extensive computational experiments, implementing a modular approach that maximizes model performance. By utilizing PyTorch's optimization utilities,and dynamic training loops, we created a neural network capable of precise multi-class character classification across the EMNIST dataset, demonstrating the power of sophisticated hyperparameter optimization strategies.</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">[1] “Tuning of CNN Architecture by CSA for EMNIST Data,” in Advances in Information Communication Technology and Computing Proceedings of AICTC 2019, </h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>The reference solution demonstrates a sophisticated approach to optimizing CNN architecture for the challenging EMNIST dataset, achieving a significant accuracy improvement from 95.4% to 98.7% through Clonal Search Algorithm (CSA) parameter tuning. Their methodology involved meticulous hyperparameter optimization, including adjusting critical parameters such as number of epochs (78), batch size (125), number of convolution layers (6), filter sizes (4x4), and network configuration. The key innovation lies in their systematic approach to hyperparameter tuning, which transformed a conventional CNN with 32 kernels of size 5x5 into a more precise model. By employing CSA optimization, they successfully enhanced the model's performance, highlighting the critical importance of intelligent hyperparameter selection in achieving superior classification accuracy on complex datasets like EMNIST. Whereas the teams accuracy with selected hyperparameters was 87.90%. </th>\n",
    "        <th>The main difference was in the amount of epochs and other computationally demanding parameters that they could maximize/ optimize. With a simple CNN with standard hyperparameter values, and 5 epochs, an accuracy for 87.90% was achieved by the teams solution. The teams model was kept computationally light to be able to train the model on regular team/user laptops/PCs. </th>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Team Solution - Fill in Descriptive Name</h3>\n",
    "\n",
    "<p>Fill in description of team solution</p>\n",
    "\n",
    "<h4 style=\"color:darkblue\">Paper/Informal Reference</h4>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"color: darkgreen\"><bf>Comparison with Reference Solution</bf></th>\n",
    "        <th style=\"color: darkgreen\"><bf>Reasons why the performance of the team's solution is better/same/worse</bf></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Fill in reference description solution and performance, and comparison between team's solution and the reference's solution</th>\n",
    "        <th>Fill in reasons for the performance differences or lack of differences between the reference and team solutions </th>\n",
    "    </tr>\n",
    "</table>\n",
    "...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h3 style=\"color:maroon\">Discussion/Summary of Solution Results</h3>\n",
    "\n",
    "<p>Please use this section to use items, such as well-labeled graphs and tables, to show a comparison of all team member solutions against each other and with the paper/informal reference solutions, to show a summary of all solution results, and give a discussion of what could be planned to be done for a hypothetical next submission to continue to improve the performance of each team member's solution.</p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4558d31-1388-4d5d-9d4e-fda03a5857af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (3,) and arg 1 with shape (4,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Plot team member accuracy bars\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m bars1 \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbar_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbar_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskyblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTeam Member Accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Plot reference accuracy bars\u001b[39;00m\n\u001b[0;32m     20\u001b[0m bars2 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m+\u001b[39m bar_width \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, reference_accuracy, width\u001b[38;5;241m=\u001b[39mbar_width, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReference Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\puttu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\pyplot.py:2956\u001b[0m, in \u001b[0;36mbar\u001b[1;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mbar)\n\u001b[0;32m   2946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbar\u001b[39m(\n\u001b[0;32m   2947\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BarContainer:\n\u001b[1;32m-> 2956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\puttu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\puttu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:2520\u001b[0m, in \u001b[0;36mAxes.bar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2518\u001b[0m         yerr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_dx(yerr, y0, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits)\n\u001b[1;32m-> 2520\u001b[0m x, height, width, y, linewidth, hatch \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make args iterable too.\u001b[39;49;00m\n\u001b[0;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;66;03m# Now that units have been converted, set the tick locations.\u001b[39;00m\n\u001b[0;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orientation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\puttu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_stride_tricks_impl.py:558\u001b[0m, in \u001b[0;36mbroadcast_arrays\u001b[1;34m(subok, *args)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# nditer is not used here to avoid the limit of 32 arrays.\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# Otherwise, something like the following one-liner would suffice:\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# return np.nditer(args, flags=['multi_index', 'zerosize_ok'],\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m#                  order='C').itviews\u001b[39;00m\n\u001b[0;32m    556\u001b[0m args \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(_m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39msubok) \u001b[38;5;28;01mfor\u001b[39;00m _m \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m--> 558\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m result \u001b[38;5;241m=\u001b[39m [array \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape\n\u001b[0;32m    561\u001b[0m           \u001b[38;5;28;01melse\u001b[39;00m _broadcast_to(array, shape, subok\u001b[38;5;241m=\u001b[39msubok, readonly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    562\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\puttu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_stride_tricks_impl.py:433\u001b[0m, in \u001b[0;36m_broadcast_shape\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the shape of the arrays that would result from broadcasting the\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03msupplied arrays against each other.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# consistently\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# unfortunately, it cannot handle 32 or more arguments directly\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;28mlen\u001b[39m(args), \u001b[38;5;241m31\u001b[39m):\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;66;03m# ironically, np.broadcast does not properly handle np.broadcast\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;66;03m# objects (it treats them as scalars)\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# use broadcasting to avoid allocating the full array\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (3,) and arg 1 with shape (4,)."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data for team member performance\n",
    "team_members = ['Junghwan Bae', 'Harshit Bhatta', 'Ethan Hoffman']\n",
    "accuracy = [85, 87.90, 78,85.67]  # Example accuracies for each team member\n",
    "reference_accuracy = [90, 98.7, 80,99.5]  # Example reference accuracies for comparison\n",
    "\n",
    "# Bar width for slim bars\n",
    "bar_width = 0.2\n",
    "# Positions for bars\n",
    "x = np.arange(len(team_members))\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot team member accuracy bars\n",
    "bars1 = plt.bar(x - bar_width / 2, accuracy, width=bar_width, color='skyblue', label='Team Member Accuracy')\n",
    "# Plot reference accuracy bars\n",
    "bars2 = plt.bar(x + bar_width / 2, reference_accuracy, width=bar_width, color='orange', label='Reference Accuracy')\n",
    "\n",
    "# Add labels on top of each bar\n",
    "for bar in bars1:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{bar.get_height():.2f}%', ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{bar.get_height():.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Team Members')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy Comparison Across Team Member Solutions', pad=20)  # Adjust title position\n",
    "plt.xticks(x, team_members)\n",
    "plt.ylim(0, 100)  # Setting y-axis from 0 to 100\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac446cf7-469f-4e97-a557-ef827cf49ae6",
   "metadata": {},
   "source": [
    " <h1 style=\"color: teal\">Discussion for further improvement for Team Solutions: </h1>\n",
    "\n",
    "#### 1. Harshit Bhatta: More epochs could be performed on the training of the model by making use of the HPCC, and hyperparameters could be optimized after analyzing the optimal number of epochs, to then tune other parameters to support this direction of improvement for futher development/improvement of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e79bd9-e8dc-4769-b9ab-d709b16918a6",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"color: darkgoldenrod\">Software</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beed320-23a0-4664-9483-b0e626619d81",
   "metadata": {},
   "source": [
    "<p>Please be sure to submit a requirements.txt file so that the instructors can easily run the code in this Jupyter Notebook.  The requirements.txt file should be in the same working directory as the Jupyter Notebook.  Also, please give any special instructions beyond the normal running of code in a Jupyter Notebook, such as where data files should be placed if not in the working directory of this Jupyter Notebook or if some of the installed software packages have additional requirements beyond \"pip install\".</p>\n",
    "\n",
    "<p>Each code cell should have contextually related code, such as a class, function implementing a major algorithm, or a set of short functions that support a larger function in a subsequent cell.  Code cells should also be present to show the performance/evaluation of a solution through well labeled graphs, tables, and/or performance measure values.</p>\n",
    "\n",
    "<p>The code cells also can be organized by each team member's solution.</p>\n",
    "\n",
    "<p>Each major set of related code cells should have the purpose of the code cells, the paper/informal references used (if any) to develop the code in the code cells, the team members who worked on the code cells, and major changes made to the code in the code cells by team members for this submission.  Changes made in a previous submission should not be included.</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2 style=\"color: teal\">Are there any special instructions for running the code in this Jupyter Notebook for this submission?</h2>\n",
    "\n",
    "<ul>\n",
    "    <li>make sure python is up to the latest, and pip has been updated succsessfully.</li>\n",
    "    <li>Make sure torch torchvision pandas matplotlib are installed</li>\n",
    "    <li> All data sets have been populated and loaded correctly and install kaggle within python environment and if using the integrated GPU make sure the correct drivers and python are installed correctly.</li>\n",
    "</ul>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e6faba-e744-4216-a5a7-55834d8773b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to capture all of the installed packages so far (run by the team to submit with the Jupyter notebook)\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9b9ece-5297-4cb8-a663-7587f16459d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "#to install all of the packages in requirements.txt (run by the instructors when grading the notebook)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298cad8-bed1-4d45-aeb2-e6ce5df3a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to import to run the code in the Jupyter Notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565039db-a366-44bf-adf9-f5928380d74a",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Hyperparameters Tuned CNN model for EMNIST</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>This is the source code of this specific team solution to demonstrate a CNN model with tuned hyperparameters to attempt to recognize the characters in the balanced dataset of EMNIST.</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>A. Remaida, A. Moumen, Y. El Bouzekri El Idrissi, and B. Abdellaoui, “Tuning convolutional neural networks hyperparameters for offline handwriting recognition,” Proceedings of the 2nd International Conference on Big Data, Modelling and Machine Learning, pp. 71–76, 2021. doi:10.5220/0010728600003101</li>\n",
    "    <li>Frank C. Eneh, \"Implementing Image Classification Algorithms,\" Medium, Nov. 22, 2024. https://medium.com/@enendufrankc/implementing-image-classification-algorithms-b052082890c0 (accessed Nov. 22, 2024).</li>\n",
    "    <li>[1] “Tuning of CNN Architecture by CSA for EMNIST Data,” in Advances in Information Communication Technology and Computing Proceedings of AICTC 2019,</li>\n",
    "    <li>[1] “CNN: Convolutional Neural Networks Explained - Computerphile,” YouTube, https://www.youtube.com/watch?v=py5byOOHZM8 (accessed Nov. 26, 2024).</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Harshit Bhatta</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/19/2024 - Changed from Tensorflow to Pytorch for model design  and implementation</li>\n",
    "    <li>11/24/2024 - Made final changes for submission: commenting, recheck, and reevaluate.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9222d7b5-d807-4ac8-8bef-f9a777d412fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels shape: torch.Size([64])\n",
      "Labels range: 0 to 46\n",
      "EMNIST_CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1152, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=47, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "[1, 100] loss: 3.126\n",
      "[1, 200] loss: 1.546\n",
      "[1, 300] loss: 1.128\n",
      "[1, 400] loss: 1.004\n",
      "[1, 500] loss: 0.890\n",
      "[1, 600] loss: 0.830\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     96\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     98\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;66;03m# Zero the parameter gradients. Resetting this for each batch. \u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:915\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m     )\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m--> 915\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    918\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "\n",
    "# importing the required modules \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Defining transformation to normalize the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))     # EMNIST mean and std\n",
    "])\n",
    "\n",
    "\n",
    "#sectioning/ preparing test/train datasets with appropriate transforms\n",
    "train_dataset = datasets.EMNIST(\n",
    "    root='./data',  # Where to store the dataset\n",
    "    split='balanced',  # Choose the split you want\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.EMNIST(\n",
    "    root='./data',\n",
    "    split='balanced',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,  # You can adjust this batch size\n",
    "    shuffle=True    # Shuffle training data\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False   # No need to shuffle test data\n",
    ")\n",
    "\n",
    "# Checking the tensors\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch shape: {images.shape}\")       \n",
    "    print(f\"Labels shape: {labels.shape}\")     \n",
    "    print(f\"Labels range: {labels.min()} to {labels.max()}\")\n",
    "    break  # Checking the first batch\n",
    "\n",
    "\n",
    "# Define the Model\n",
    "class EMNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)        # Defining Convolutional Layers and Setting hyperparameters\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)                    # Using MaxPool layer\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)  # Adjusting for 28x28 input size for the dense layer after convolutional and pooling layers\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 47)  # output class is 47\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))            #Pooling layer after ReLU activation\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 3 * 3)               # Flatten the tensor from (batch_size, 128, 3, 3) to (batch_size, 128 * 3 * 3) for dense layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = EMNIST_CNN()\n",
    "print(model)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()       #Using Cross Entropy as the loss function \n",
    "\n",
    "# Using Adam optimizer for better performance and adaptive learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimal learning rate \"Alpha\" for Adam.  \n",
    "\n",
    "# Number of epochs to train the model. Could be increased according to users need for accuracy and available compute power.\n",
    "num_epochs = 5                          \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients. Resetting this for each batch. \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "# Saving the model to future use with test data/ not burn the training device\n",
    "torch.save(model.state_dict(), 'to_save_model.pth')\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "\n",
    "# Function to load the model later\n",
    "def load_model():\n",
    "    loaded_model = EMNIST_CNN()  # Create a new instance of the model\n",
    "    loaded_model.load_state_dict(torch.load('to_save_model.pth'))\n",
    "    loaded_model.eval()  # Set to evaluation mode\n",
    "    return loaded_model\n",
    "\n",
    "# Evaluate the loaded model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation, not needed for saved model\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "# Function to visualize results\n",
    "def visualize_results(model, test_loader, num_images=10):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    images, labels = next(iter(test_loader))  # Get a batch of test images\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot the images with predicted and actual labels\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(f\"Pred: {predicted[i].item()}\\nActual: {labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the model, evaluate it, and plot its results\n",
    "loaded_model = load_model()\n",
    "evaluate_model(loaded_model, test_loader)\n",
    "visualize_results(loaded_model, test_loader)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1d631-cc96-4958-a65e-d0f32785b506",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Descriptive Name</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>fill in the purpose of the code cell block</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>paper/informal reference</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Team members contributing to the code cell block</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>mm/dd/yyyy - major change made for this submission</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>mm/dd/yyyy - major change made for this submission</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b45c6-a6d2-4dcf-9941-6e15bb2697b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44024dbf-a49d-4901-8d34-cab79c43429c",
   "metadata": {},
   "source": [
    "<h3 style=\"color:maroon\">Code Cell Block - Descriptive Name</h3>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Purpose</h4>\n",
    "\n",
    "<p>The purpose is to show all the reference sites I used for developing and constructing this model and the different representations of how the model should be represented as, as well constructing and utilizing the CNN with different attributes to help the model learn.</p>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">References used to develop the code</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>paper/informal reference</li>\n",
    "    <li>[1]OpenAI, “Introducing ChatGPT,” Openai.com, Nov. 30, 2022. https://openai.com/index/chatgpt/</li>\n",
    "    <li>[1]“torcheval.metrics.functional.multiclass_f1_score — TorchEval main documentation,” Pytorch.org, 2022. https://pytorch.org/torcheval/stable/generated/torcheval.metrics.functional.multiclass_f1_score.html (accessed Nov. 23, 2024).\n",
    "    </li>\n",
    "    <li>[1]GeeksforGeeks, “How to squeeze and unsqueeze a tensor in PyTorch?,” GeeksforGeeks, Mar. 26, 2022. https://www.geeksforgeeks.org/how-to-squeeze-and-unsqueeze-a-tensor-in-pytorch/ (accessed Nov. 23, 2024).\n",
    "‌   </li>\n",
    "    <li>[1]A. Tam, “Handwritten Digit Recognition with LeNet5 Model in PyTorch - MachineLearningMastery.com,” MachineLearningMastery.com, Mar. 07, 2023. https://machinelearningmastery.com/handwritten-digit-recognition-with-lenet5-model-in-pytorch/\n",
    "‌   </li>\n",
    "    <li>[1]“Adam — PyTorch 1.11.0 documentation,” pytorch.org. https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "‌ </li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color:darkgreen\">Sagar Basavaraju</h4>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>11/21/2023 - Found 2 informal and 2 formal research articles regarding CNN networks and different applications and precision values regarding how we can analyze predict and validate and better train our current model</li>\n",
    "    <li>Extensively researching of libraries and optimizations bring important metrics such as F1 Score,accuracy. Using differentiating between labeling and unlabeled data and simulating unlabeled data and extensivly using metrics from sklearn and creating y-pred and y-true values and detailed prediction analyzes based on how well support was compared to true labeled data. </li>\n",
    "</ul>\n",
    "\n",
    "<h5 style=\"color:darkblue\">Team member name</h5>\n",
    "\n",
    "<ul>\n",
    "    <li>mm/dd/yyyy - major change made for this submission</li>\n",
    "    <li>...</li>\n",
    "</ul>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3f9cba-98c2-4c51-9a4f-488c07be834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 153.0509\n",
      "Epoch [2/4], Loss: 41.4978\n",
      "Epoch [3/4], Loss: 29.1041\n",
      "Epoch [4/4], Loss: 24.2825\n",
      "Accuracy if no labels: 0.10%\n",
      "F1-Score if no labels: 0.10%\n",
      "Predictions (first 10): [2, 0, 9, 0, 3, 7, 0, 3, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "#code cells for running and evaluating the solution\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "import numpy as np\n",
    "# Custom Dataset\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, name, transform=ToTensor(), label_name=\"label\", is_labeled=True):\n",
    "        self.data = pd.read_csv(name)  \n",
    "        self.transform = transform\n",
    "        self.label_name = label_name\n",
    "        self.is_labeled = is_labeled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pixel_data = self.data.iloc[idx].drop(self.label_name, errors='ignore').values.astype('float32')\n",
    "        scaled_pixel = (pixel_data - pixel_data.min()) / (pixel_data.max() - pixel_data.min())\n",
    "        image = scaled_pixel.reshape(28, 28)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_labeled:\n",
    "            label = int(self.data.iloc[idx][self.label_name])\n",
    "            return image, label\n",
    "        return image\n",
    "\n",
    "# Load Datasets\n",
    "train_dataset = CustomMNISTDataset(name=\"train.csv\", is_labeled=True)\n",
    "test_dataset = CustomMNISTDataset(name=\"test.csv\", is_labeled=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# CNN Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(64 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Training\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        if len(images.shape) != 4:\n",
    "            images = images.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "correct = 0\n",
    "if test_dataset.is_labeled:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if len(images.shape) != 4:\n",
    "                images = images.unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            y_pred.extend(predictions.cpu().numpy().tolist())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "        accuracy = accuracy_score(y_true,y_pred)\n",
    "        F1_score = f1_score(y_true,y_pred,average=\"weighted\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"F1-Score: {F1_score:.2f}%\")\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            if len(images.shape) != 4:\n",
    "                images = images.unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            y_pred.extend(predictions.cpu().numpy().tolist())\n",
    "    #simulations if labels were provided:\n",
    "    nlabels = np.random.choice(10,len(y_pred)) \n",
    "    accuracy = accuracy_score(nlabels,y_pred)\n",
    "    F1_score = f1_score(nlabels,y_pred,average=\"weighted\")\n",
    "    print(f\"Accuracy if no labels: {accuracy:.2f}%\")\n",
    "    print(f\"F1-Score if no labels: {F1_score:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"Predictions (first 10): {y_pred[:10]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
